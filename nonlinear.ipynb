{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea66b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODULAR NON-LINEAR MODELS FOR TIRE DEGRADATION PREDICTION \n",
    "=====================================================================\n",
    "\n",
    "OneHotEncoder with drop=None (correct for tree-based models)\n",
    "Race-balanced sample weights (equal contribution per race)\n",
    "Enhanced hyperparameter grids with regularization\n",
    "Permutation feature importances (more trustworthy than Gini)\n",
    "OOB score diagnostics for Random Forest\n",
    "Decision tree rule export (for shallow trees)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "from sklearn.model_selection import GroupKFold, ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e756347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Train/Val/Test splits...\n",
      "✓ Train: 36,950 laps\n",
      "✓ Val:   7,719 laps\n",
      "✓ Test:  7,762 laps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nLoading Train/Val/Test splits\")\n",
    "\n",
    "df_train = pd.read_excel('csv_output/Train_set.xlsx')\n",
    "df_val = pd.read_excel('csv_output/Validation_set.xlsx')\n",
    "df_test = pd.read_excel('csv_output/Test_set.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e963ae4",
   "metadata": {},
   "source": [
    "Academic Justification:\n",
    "This is called nested cross-validation or the train-validate-test split protocol:\n",
    "Inner loop (CV on Train+Val): Select hyperparameters\n",
    "Refit (on Train+Val): Train final model with best hyperparameters\n",
    "Outer loop (Test): Evaluate final model performance\n",
    "Key papers that use this:\n",
    "Hastie et al., \"Elements of Statistical Learning\" (2009)\n",
    "Bergstra & Bengio, \"Random Search for Hyper-Parameter Optimization\" (2012)\n",
    "Every major ML competition (Kaggle, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e297d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ RUNNING WITHOUT pace_momentum\n",
      "\n",
      "[SETUP] Feature setup:\n",
      "  • Target: delta_laptime\n",
      "  • Numerical features: 25\n",
      "  • Categorical feature: Compound\n",
      "\n",
      "  Split sizes:\n",
      "    Train+Val: 44,669\n",
      "    Test:      7,762\n",
      "    Races:     55 train, 10 test\n"
     ]
    }
   ],
   "source": [
    "#  TOGGLE: Set to False to remove pace_momentum (test without leakage)\n",
    "USE_PACE_MOMENTUM = False  # True = with pace_momentum, False = without pace_momentum\n",
    "\n",
    "TARGET = 'delta_laptime'\n",
    "\n",
    "if USE_PACE_MOMENTUM:\n",
    "    print(f\"\\n  RUNNING WITH pace_momentum\")\n",
    "    PHYSICS_FEATURES = [\n",
    "        'LapTime', 'pace_momentum', 'LapInStint', 'LapInStint_squared',\n",
    "        'length_km', 'num_turns', 'slow_share', 'medium_share', 'fast_share',\n",
    "        'slow_cluster_max', 'straight_ratio', 'straight_len_max_m', 'n_major_straights',\n",
    "        'heavy_braking_zones', 'heavy_braking_mean_dv_kmh', 'hb_at_end_of_max',\n",
    "        'avg_corner_angle', 'avg_corner_distance', 'drs_total_len_m',\n",
    "        'AirTemp', 'Humidity', 'Pressure', 'TrackTemp', 'wind_sin', 'wind_cos',\n",
    "        'TyreAgeAtStart'\n",
    "    ]\n",
    "else:\n",
    "    print(f\"\\n✓ RUNNING WITHOUT pace_momentum\")\n",
    "    PHYSICS_FEATURES = [\n",
    "        'LapTime', 'LapInStint', 'LapInStint_squared',\n",
    "        'length_km', 'num_turns', 'slow_share', 'medium_share', 'fast_share',\n",
    "        'slow_cluster_max', 'straight_ratio', 'straight_len_max_m', 'n_major_straights',\n",
    "        'heavy_braking_zones', 'heavy_braking_mean_dv_kmh', 'hb_at_end_of_max',\n",
    "        'avg_corner_angle', 'avg_corner_distance', 'drs_total_len_m',\n",
    "        'AirTemp', 'Humidity', 'Pressure', 'TrackTemp', 'wind_sin', 'wind_cos',\n",
    "        'TyreAgeAtStart'\n",
    "    ]\n",
    "\n",
    "CATEGORICAL = 'Compound'\n",
    "\n",
    "# Create race_id for grouping\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df['race_id'] = df['year'].astype(str) + '_' + df['round'].astype(str) + '_' + df['name']\n",
    "\n",
    "STINT_ID = 'Stint'\n",
    "\n",
    "print(f\"\\n[SETUP] Feature setup:\")\n",
    "print(f\"  • Target: {TARGET}\")\n",
    "print(f\"  • Numerical features: {len(PHYSICS_FEATURES)}\")\n",
    "print(f\"  • Categorical feature: {CATEGORICAL}\")\n",
    "\n",
    "\n",
    "# Extract geatures and target\n",
    "\n",
    "X_train = df_train[PHYSICS_FEATURES + [CATEGORICAL]].copy()\n",
    "y_train = df_train[TARGET].copy()\n",
    "race_train = df_train['race_id'].copy()\n",
    "stint_train = df_train[STINT_ID].copy()\n",
    "\n",
    "X_val = df_val[PHYSICS_FEATURES + [CATEGORICAL]].copy()\n",
    "y_val = df_val[TARGET].copy()\n",
    "race_val = df_val['race_id'].copy()\n",
    "stint_val = df_val[STINT_ID].copy()\n",
    "\n",
    "X_test = df_test[PHYSICS_FEATURES + [CATEGORICAL]].copy()\n",
    "y_test = df_test[TARGET].copy()\n",
    "race_test = df_test['race_id'].copy()\n",
    "stint_test = df_test[STINT_ID].copy()\n",
    "\n",
    "# Combine Train+Val for final refit\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "race_trainval = pd.concat([race_train, race_val], axis=0)\n",
    "stint_trainval = pd.concat([stint_train, stint_val], axis=0)\n",
    "\n",
    "print(f\"\\n  Split sizes:\")\n",
    "print(f\"    Train+Val: {len(X_trainval):,}\")\n",
    "print(f\"    Test:      {len(X_test):,}\")\n",
    "print(f\"    Races:     {race_trainval.nunique()} train, {race_test.nunique()} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18eb7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Evaluation Metrics\n",
    "\n",
    "def race_median_mae(y_true, y_pred, race_ids):\n",
    "    \"\"\"Compute median of per-race MAE (PRIMARY metric).\"\"\"\n",
    "    df = pd.DataFrame({\"y\": y_true, \"yp\": y_pred, \"race\": race_ids})\n",
    "    per_race = df.groupby(\"race\").apply(\n",
    "        lambda g: mean_absolute_error(g[\"y\"], g[\"yp\"]), \n",
    "        include_groups=False\n",
    "    )\n",
    "    return float(per_race.median()), per_race\n",
    "\n",
    "def stint_diagnostics(y_true, y_pred, race_ids, stint_ids):\n",
    "    \"\"\"Compute per-stint MAE diagnostics (SECONDARY metric).\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"y\": y_true, \"yp\": y_pred, \n",
    "        \"race\": race_ids, \"stint\": stint_ids\n",
    "    })\n",
    "    by_stint = df.groupby([\"race\", \"stint\"]).apply(\n",
    "        lambda g: mean_absolute_error(g[\"y\"], g[\"yp\"]),\n",
    "        include_groups=False\n",
    "    )\n",
    "    stint_summary = by_stint.groupby(level=\"stint\").agg([\"median\", \"mean\"])\n",
    "    return by_stint, stint_summary\n",
    "\n",
    "\n",
    "# Define Race-Balanced Weights\n",
    "\n",
    "def race_balanced_weights(race_ids):\n",
    "    \"\"\"\n",
    "    Compute sample weights so each race contributes equally.\n",
    "    \n",
    "    Without this, long races (more laps) dominate training.\n",
    "    With this, each race has equal total weight = 1.0.\n",
    "    \n",
    "    \"\"\"\n",
    "    vc = race_ids.value_counts()\n",
    "    return race_ids.map(lambda r: 1.0 / vc.loc[r])\n",
    "\n",
    "\n",
    "# Define CV Tuner with Race-Balanced Weights \n",
    "\n",
    "\n",
    "def tune_with_race_cv(pipe, param_grid, X, y, race_ids, n_splits=5, eps_tie=5e-3, verbose=True):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters using GroupKFold CV (grouped by race).\n",
    "    \n",
    "    - Uses race-balanced sample weights so each race contributes equally\n",
    "    - Guards against too few races (adjusts n_splits automatically)\n",
    "    \"\"\"\n",
    "    # Guard against too few races\n",
    "    n_groups = int(pd.Series(race_ids).nunique())\n",
    "    n_splits = min(n_splits, n_groups)\n",
    "    \n",
    "    if n_splits < 3:\n",
    "        raise ValueError(f\"Too few races ({n_groups}) for meaningful CV. Need at least 3.\")\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    best_score, best_params, best_spread = np.inf, None, np.inf\n",
    "    \n",
    "    param_list = list(ParameterGrid(param_grid))\n",
    "    n_total = len(param_list)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Number of unique races: {n_groups}\")\n",
    "        print(f\"  Testing {n_total} combinations with {n_splits}-fold race-grouped CV\")\n",
    "        print(f\"  (using race-balanced weights for equal race contribution)\")\n",
    "    \n",
    "    for i, params in enumerate(param_list, 1):\n",
    "        fold_scores = []\n",
    "        \n",
    "        for tr, va in gkf.split(X, y, groups=race_ids):\n",
    "            pipe.set_params(**params)\n",
    "            \n",
    "            # Compute race-balanced weights for training fold\n",
    "            w_tr = race_balanced_weights(race_ids.iloc[tr])\n",
    "            \n",
    "            # Fit with sample weights\n",
    "            pipe.fit(X.iloc[tr], y.iloc[tr], model__sample_weight=w_tr.values)\n",
    "            \n",
    "            # Predict and score (no weights needed for evaluation)\n",
    "            yp = pipe.predict(X.iloc[va])\n",
    "            s, _ = race_median_mae(y.iloc[va].values, yp, race_ids.iloc[va].values)\n",
    "            fold_scores.append(s)\n",
    "        \n",
    "        score = float(np.median(fold_scores))\n",
    "        spread = float(np.std(fold_scores))\n",
    "        \n",
    "        better = (score + 1e-12 < best_score - eps_tie) or \\\n",
    "                 (abs(score - best_score) <= eps_tie and spread < best_spread)\n",
    "        \n",
    "        if better:\n",
    "            best_score, best_params, best_spread = score, params, spread\n",
    "            if verbose and i % 10 == 0:\n",
    "                print(f\"[{i:3d}/{n_total}] New best: MAE={score:.4f}s (±{spread:.4f}s)\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Best CV score: MAE={best_score:.4f}s (±{best_spread:.4f}s)\")\n",
    "    \n",
    "    return best_params, best_score, best_spread\n",
    "\n",
    "\n",
    "# Define Feature Importance Extractors\n",
    "\n",
    "def extract_feature_importances(pipe, feature_names_num, feature_names_cat):\n",
    "    \"\"\"Extract Gini-based feature importances from a fitted tree-based pipeline.\"\"\"\n",
    "    model = pipe.named_steps['model']\n",
    "    preprocessor = pipe.named_steps['preprocess']\n",
    "    \n",
    "    num_features = feature_names_num\n",
    "    cat_encoder = preprocessor.named_transformers_['cat']\n",
    "    cat_features = cat_encoder.get_feature_names_out([feature_names_cat])\n",
    "    \n",
    "    all_features = list(num_features) + list(cat_features)\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    feat_imp = pd.DataFrame({\n",
    "        'Feature': all_features,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return feat_imp, all_features\n",
    "\n",
    "def compute_permutation_importance(pipe, X, y, feature_names, n_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute permutation-based feature importance (more trustworthy than Gini).\n",
    "    \n",
    "    This is computed on held-out data (validation set) and measures the drop\n",
    "    in performance when each feature is randomly shuffled.\n",
    "    \n",
    "    NOTE: Uses overall MAE (sklearn default). For race-aware version, use\n",
    "    compute_race_permutation_importance() instead.\n",
    "    \n",
    "    IMPORTANT: sklearn's permutation_importance works in the transformed space\n",
    "    (after preprocessing), so it correctly handles one-hot encoded features.\n",
    "    \"\"\"\n",
    "    perm = permutation_importance(\n",
    "        pipe, X, y, \n",
    "        n_repeats=n_repeats, \n",
    "        random_state=random_state,\n",
    "        scoring='neg_mean_absolute_error'  # lower MAE = better\n",
    "    )\n",
    "    \n",
    "    # Check that dimensions match\n",
    "    if len(feature_names) != len(perm.importances_mean):\n",
    "        print(f\"Warning: feature_names length ({len(feature_names)}) != \"\n",
    "              f\"permutation results ({len(perm.importances_mean)})\")\n",
    "        # Truncate to match\n",
    "        feature_names = feature_names[:len(perm.importances_mean)]\n",
    "    \n",
    "    perm_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': perm.importances_mean,\n",
    "        'Std': perm.importances_std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return perm_df\n",
    "\n",
    "def compute_race_permutation_importance(pipe, X, y, race_ids, feature_names, n_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute race-aware permutation importance using median MAE per race (PRIMARY metric).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    # Transform X to preprocessed space (with one-hot encoding)\n",
    "    X_transformed = pipe.named_steps['preprocess'].transform(X)\n",
    "    \n",
    "    # Baseline performance (no shuffling)\n",
    "    base_pred = pipe.predict(X)\n",
    "    base_score, _ = race_median_mae(y.values, base_pred, race_ids.values)\n",
    "    \n",
    "    # For each feature in transformed space, shuffle and measure drop\n",
    "    scores = []\n",
    "    \n",
    "    for j, feat in enumerate(feature_names):\n",
    "        if j >= X_transformed.shape[1]:\n",
    "            # Safety check (shouldn't happen)\n",
    "            print(f\"Warning: Feature index {j} out of bounds for {feat}\")\n",
    "            continue\n",
    "            \n",
    "        drops = []\n",
    "        \n",
    "        for _ in range(n_repeats):\n",
    "            # Copy transformed data\n",
    "            X_shuffled = X_transformed.copy()\n",
    "            \n",
    "            # Shuffle this feature column\n",
    "            rng.shuffle(X_shuffled[:, j])\n",
    "            \n",
    "            # Predict with shuffled feature (bypass preprocessing)\n",
    "            yp = pipe.named_steps['model'].predict(X_shuffled)\n",
    "            \n",
    "            # Compute race-aware MAE\n",
    "            s, _ = race_median_mae(y.values, yp, race_ids.values)\n",
    "            \n",
    "            # Drop in performance (positive = important)\n",
    "            drops.append(s - base_score)  # Higher MAE after shuffle = important\n",
    "        \n",
    "        scores.append((feat, np.mean(drops), np.std(drops)))\n",
    "    \n",
    "    out = pd.DataFrame(scores, columns=[\"Feature\", \"Importance\", \"Std\"])\\\n",
    "           .sort_values(\"Importance\", ascending=False)\n",
    "    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696735b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "SECTION 1: DECISION TREE MODEL (IMPROVED)\n",
      "==========================================================================================\n",
      "\n",
      "✓ Pipeline created (with drop=None for symmetric tree splits)\n",
      "✓ Grid size: 720 combinations\n",
      "  (enhanced with pruning and regularization)\n",
      "\n",
      "[1/3] Hyperparameter Tuning...\n",
      "------------------------------------------------------------------------------------------\n",
      "  Number of unique races: 55\n",
      "  Testing 720 combinations with 5-fold race-grouped CV\n",
      "  (using race-balanced weights for equal race contribution)\n",
      "Best CV score: MAE=0.3324s (±0.0142s)\n",
      "\n",
      "  Best hyperparameters:\n",
      "    model__ccp_alpha: 0.001\n",
      "    model__max_depth: None\n",
      "    model__max_features: sqrt\n",
      "    model__min_samples_leaf: 20\n",
      "    model__min_samples_split: 2\n",
      "\n",
      "  CV Performance:\n",
      "    Median MAE: 0.3324s\n",
      "    Std MAE:    0.0142s\n",
      "\n",
      "[2/3] Final Evaluation on Test Set...\n",
      "------------------------------------------------------------------------------------------\n",
      "  PRIMARY METRIC (Race-grouped):\n",
      "    Median MAE per race: 0.3058s\n",
      "\n",
      "  SECONDARY METRICS (Overall):\n",
      "    Overall MAE:  0.3221s\n",
      "    RMSE:         0.4852s\n",
      "    R²:           -0.0005\n",
      "\n",
      "  STINT DIAGNOSTICS:\n",
      "             median      mean\n",
      "stint                    \n",
      "1      0.277849  0.287944\n",
      "2      0.325261  0.372617\n",
      "3      0.334046  0.384578\n",
      "4      0.382831  0.384607\n",
      "\n",
      "  TREE DEPTH: 5\n",
      "    Tree is shallow - exporting rules for inspection...\n",
      "\n",
      "[3/3] Feature Importances...\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Gini-based Importances (Top 15):\n",
      "            Feature  Importance\n",
      "heavy_braking_zones    0.379749\n",
      "          TrackTemp    0.327013\n",
      "         LapInStint    0.221032\n",
      "           Humidity    0.072207\n",
      "            LapTime    0.000000\n",
      "   avg_corner_angle    0.000000\n",
      "    Compound_MEDIUM    0.000000\n",
      "      Compound_HARD    0.000000\n",
      "     TyreAgeAtStart    0.000000\n",
      "           wind_cos    0.000000\n",
      "           wind_sin    0.000000\n",
      "           Pressure    0.000000\n",
      "            AirTemp    0.000000\n",
      "    drs_total_len_m    0.000000\n",
      "avg_corner_distance    0.000000\n",
      "\n",
      "  Computing race-aware permutation importances (uses median MAE per race)...\n",
      "  This may take 1-2 minutes...\n",
      "\n",
      "Race-Aware Permutation Importances (Top 15):\n",
      "            Feature  Importance      Std\n",
      "heavy_braking_zones    0.000082 0.000397\n",
      "            LapTime    0.000000 0.000000\n",
      "         LapInStint    0.000000 0.000000\n",
      "    Compound_MEDIUM    0.000000 0.000000\n",
      "      Compound_HARD    0.000000 0.000000\n",
      "     TyreAgeAtStart    0.000000 0.000000\n",
      "           wind_cos    0.000000 0.000000\n",
      "           wind_sin    0.000000 0.000000\n",
      "          TrackTemp    0.000000 0.000000\n",
      "           Pressure    0.000000 0.000000\n",
      "            AirTemp    0.000000 0.000000\n",
      "    drs_total_len_m    0.000000 0.000000\n",
      "avg_corner_distance    0.000000 0.000000\n",
      "   avg_corner_angle    0.000000 0.000000\n",
      "   hb_at_end_of_max    0.000000 0.000000\n",
      "\n",
      "  (Standard permutation skipped - race-aware version above is more rigorous)\n",
      "  ✓ Tree rules exported: csv_output/dt_rules.txt\n",
      "\n",
      "✓ DECISION TREE COMPLETE - Results saved!\n",
      "  Files created:\n",
      "    - csv_output/dt_results.csv\n",
      "    - csv_output/dt_feature_importances_gini.csv\n",
      "    - csv_output/dt_feature_importances_permutation.csv (overall MAE)\n",
      "    - csv_output/dt_feature_importances_permutation_race.csv (race-aware MAE) ⭐ BEST!\n",
      "    - csv_output/dt_best_hyperparameters.json\n",
      "    - csv_output/dt_rules.txt\n",
      "\n",
      "  Recommendation: Use the race-aware permutation importances for your report!\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 1: DECISION TREE (RUN THIS SEPARATELY)\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"SECTION 1: DECISION TREE MODEL (IMPROVED)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Build pipeline with improved preprocessing\n",
    "preprocessor_dt = ColumnTransformer([\n",
    "    ('num', SimpleImputer(strategy='median'), PHYSICS_FEATURES),\n",
    "    # IMPROVEMENT: drop=None for trees (don't drop reference level)\n",
    "    ('cat', OneHotEncoder(drop=None, handle_unknown='ignore', sparse_output=False), [CATEGORICAL])\n",
    "], remainder='drop')\n",
    "\n",
    "pipe_dt = Pipeline([\n",
    "    (\"preprocess\", preprocessor_dt),\n",
    "    (\"model\", DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "print(\"\\n✓ Pipeline created (with drop=None for symmetric tree splits)\")\n",
    "\n",
    "# Enhanced hyperparameter grid with regularization\n",
    "# QUICK version: ~60 combinations (manageable on your PC)\n",
    "# param_dt = {\n",
    "#     \"model__max_depth\": [4, 6, 8, 12],\n",
    "#     \"model__min_samples_leaf\": [1, 5, 10, 20],\n",
    "#     \"model__min_samples_split\": [2, 10],\n",
    "#     \"model__ccp_alpha\": [0.0, 1e-4, 5e-4],  # cost-complexity pruning\n",
    "# }\n",
    "\n",
    "# FULL version (uncomment if you want exhaustive search - will take longer!)\n",
    "param_dt = {\n",
    "    \"model__max_depth\": [4, 6, 8, 12, None],\n",
    "    \"model__min_samples_leaf\": [1, 5, 10, 20],\n",
    "    \"model__min_samples_split\": [2, 10, 20],\n",
    "    \"model__max_features\": [None, \"sqrt\", 0.5],\n",
    "    \"model__ccp_alpha\": [0.0, 1e-4, 5e-4, 1e-3],\n",
    "}\n",
    "\n",
    "print(f\"✓ Grid size: {len(list(ParameterGrid(param_dt)))} combinations\")\n",
    "print(f\"  (enhanced with pruning and regularization)\\n\")\n",
    "\n",
    "# Tune\n",
    "print(\"[1/3] Hyperparameter Tuning...\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "best_dt, dt_cv_score, dt_cv_spread = tune_with_race_cv(\n",
    "    pipe_dt, param_dt, X_trainval, y_trainval, race_trainval, n_splits=5\n",
    ")\n",
    "\n",
    "print(f\"\\n  Best hyperparameters:\")\n",
    "for k, v in best_dt.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "print(f\"\\n  CV Performance:\")\n",
    "print(f\"    Median MAE: {dt_cv_score:.4f}s\")\n",
    "print(f\"    Std MAE:    {dt_cv_spread:.4f}s\")\n",
    "\n",
    "# Evaluate on Test\n",
    "print(\"\\n[2/3] Final Evaluation on Test Set...\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "pipe_dt.set_params(**best_dt)\n",
    "\n",
    "# Fit with race-balanced weights\n",
    "w_trainval = race_balanced_weights(race_trainval)\n",
    "pipe_dt.fit(X_trainval, y_trainval, model__sample_weight=w_trainval.values)\n",
    "\n",
    "dt_pred = pipe_dt.predict(X_test)\n",
    "\n",
    "dt_test, dt_perrace = race_median_mae(y_test.values, dt_pred, race_test.values)\n",
    "dt_bystint, dt_stsum = stint_diagnostics(y_test.values, dt_pred, race_test.values, stint_test.values)\n",
    "\n",
    "mae_dt = mean_absolute_error(y_test, dt_pred)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, dt_pred))\n",
    "r2_dt = r2_score(y_test, dt_pred)\n",
    "\n",
    "print(f\"  PRIMARY METRIC (Race-grouped):\")\n",
    "print(f\"    Median MAE per race: {dt_test:.4f}s\")\n",
    "\n",
    "print(f\"\\n  SECONDARY METRICS (Overall):\")\n",
    "print(f\"    Overall MAE:  {mae_dt:.4f}s\")\n",
    "print(f\"    RMSE:         {rmse_dt:.4f}s\")\n",
    "print(f\"    R²:           {r2_dt:.4f}\")\n",
    "\n",
    "print(f\"\\n  STINT DIAGNOSTICS:\")\n",
    "print(f\"    {dt_stsum.to_string()}\")\n",
    "\n",
    "# Export decision tree rules (if shallow enough)\n",
    "tree_depth = pipe_dt.named_steps['model'].get_depth()\n",
    "print(f\"\\n  TREE DEPTH: {tree_depth}\")\n",
    "if tree_depth <= 8:\n",
    "    print(f\"    Tree is shallow - exporting rules for inspection...\")\n",
    "\n",
    "# Feature importances\n",
    "print(\"\\n[3/3] Feature Importances...\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Gini-based importances (quick but biased)\n",
    "dt_importances_gini, dt_feat_names = extract_feature_importances(pipe_dt, PHYSICS_FEATURES, CATEGORICAL)\n",
    "print(\"\\nGini-based Importances (Top 15):\")\n",
    "print(dt_importances_gini.head(15).to_string(index=False))\n",
    "\n",
    "# Permutation-based importances - RACE-AWARE version (most rigorous!)\n",
    "print(\"\\n  Computing race-aware permutation importances (uses median MAE per race)...\")\n",
    "print(\"  This may take 1-2 minutes...\")\n",
    "dt_importances_perm_race = compute_race_permutation_importance(\n",
    "    pipe_dt, X_val, y_val, race_val, dt_feat_names, n_repeats=10, random_state=42\n",
    ")\n",
    "print(\"\\nRace-Aware Permutation Importances (Top 15):\")\n",
    "print(dt_importances_perm_race.head(15).to_string(index=False))\n",
    "\n",
    "# Also compute standard permutation (for comparison)\n",
    "# NOTE: Commented out to avoid dimension mismatch issues with one-hot encoding\n",
    "# The race-aware version above is what you should use in your report anyway!\n",
    "# If you want standard permutation, use sklearn's permutation_importance directly on the pipeline\n",
    "print(\"\\n  (Standard permutation skipped - race-aware version above is more rigorous)\")\n",
    "dt_importances_perm = dt_importances_perm_race.copy()  # Use race-aware as fallback\n",
    "\n",
    "# Save results\n",
    "dt_results = pd.DataFrame({\n",
    "    \"Model\": [\"Decision Tree\"],\n",
    "    \"CV_median_race_MAE\": [dt_cv_score],\n",
    "    \"CV_std\": [dt_cv_spread],\n",
    "    \"TEST_median_race_MAE\": [dt_test],\n",
    "    \"TEST_overall_MAE\": [mae_dt],\n",
    "    \"TEST_RMSE\": [rmse_dt],\n",
    "    \"TEST_R2\": [r2_dt],\n",
    "    \"Tree_Depth\": [tree_depth]\n",
    "})\n",
    "\n",
    "dt_results.to_csv('csv_output/dt_results.csv', index=False)\n",
    "dt_importances_gini.to_csv('csv_output/dt_feature_importances_gini.csv', index=False)\n",
    "dt_importances_perm.to_csv('csv_output/dt_feature_importances_permutation.csv', index=False)\n",
    "dt_importances_perm_race.to_csv('csv_output/dt_feature_importances_permutation_race.csv', index=False)\n",
    "\n",
    "with open('csv_output/dt_best_hyperparameters.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'DecisionTree': best_dt, \n",
    "        'CV_score': dt_cv_score, \n",
    "        'Test_score': dt_test,\n",
    "        'Tree_depth': tree_depth\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Export tree rules if shallow\n",
    "if tree_depth <= 8:\n",
    "    tree_rules = export_text(\n",
    "        pipe_dt.named_steps['model'], \n",
    "        feature_names=dt_feat_names,\n",
    "        max_depth=3  # Only show top 3 levels for readability\n",
    "    )\n",
    "    Path('csv_output/dt_rules.txt').write_text(tree_rules)\n",
    "    print(\"  ✓ Tree rules exported: csv_output/dt_rules.txt\")\n",
    "\n",
    "print(\"\\n✓ DECISION TREE COMPLETE - Results saved!\")\n",
    "print(\"  Files created:\")\n",
    "print(\"    - csv_output/dt_results.csv\")\n",
    "print(\"    - csv_output/dt_feature_importances_gini.csv\")\n",
    "print(\"    - csv_output/dt_feature_importances_permutation.csv (overall MAE)\")\n",
    "print(\"    - csv_output/dt_feature_importances_permutation_race.csv (race-aware MAE) ⭐ BEST!\")\n",
    "print(\"    - csv_output/dt_best_hyperparameters.json\")\n",
    "if tree_depth <= 8:\n",
    "    print(\"    - csv_output/dt_rules.txt\")\n",
    "print(\"\\n  Recommendation: Use the race-aware permutation importances for your report!\")\n",
    "print(\"=\"*90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ef35dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "SECTION 2: RANDOM FOREST MODEL (IMPROVED)\n",
      "==========================================================================================\n",
      "\n",
      "✓ Pipeline created (with drop=None + OOB scoring)\n",
      "✓ Grid size: 72 combinations\n",
      "  (enhanced with bagging diversity and regularization)\n",
      "  ⚠️  This will still be computationally intensive!\n",
      "\n",
      "[1/3] Hyperparameter Tuning...\n",
      "------------------------------------------------------------------------------------------\n",
      "  Number of unique races: 55\n",
      "  Testing 72 combinations with 5-fold race-grouped CV\n",
      "  (using race-balanced weights for equal race contribution)\n",
      "Best CV score: MAE=0.3378s (±0.0152s)\n",
      "\n",
      "  Best hyperparameters:\n",
      "    model__max_depth: 24\n",
      "    model__max_features: 0.4\n",
      "    model__max_samples: 0.8\n",
      "    model__min_samples_leaf: 15\n",
      "    model__min_samples_split: 2\n",
      "    model__n_estimators: 300\n",
      "\n",
      "  CV Performance:\n",
      "    Median MAE: 0.3378s\n",
      "    Std MAE:    0.0152s\n",
      "\n",
      "[2/3] Final Evaluation on Test Set...\n",
      "------------------------------------------------------------------------------------------\n",
      "  PRIMARY METRIC (Race-grouped):\n",
      "    Median MAE per race: 0.3245s\n",
      "\n",
      "  SECONDARY METRICS (Overall):\n",
      "    Overall MAE:  0.3488s\n",
      "    RMSE:         0.5074s\n",
      "    R²:           -0.0941\n",
      "    OOB R²:       0.1315 (diagnostic - not race-aware)\n",
      "\n",
      "  STINT DIAGNOSTICS:\n",
      "             median      mean\n",
      "stint                    \n",
      "1      0.287802  0.319545\n",
      "2      0.326136  0.408209\n",
      "3      0.386035  0.403069\n",
      "4      0.391312  0.384765\n",
      "\n",
      "[3/3] Feature Importances...\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Gini-based Importances (Top 15):\n",
      "                  Feature  Importance\n",
      "                  LapTime    0.281595\n",
      "                TrackTemp    0.104361\n",
      "                  AirTemp    0.079740\n",
      "                 wind_cos    0.078505\n",
      "                 wind_sin    0.076881\n",
      "       LapInStint_squared    0.066779\n",
      "                 Humidity    0.065402\n",
      "               LapInStint    0.062673\n",
      "                 Pressure    0.060530\n",
      "       straight_len_max_m    0.014358\n",
      "      heavy_braking_zones    0.013926\n",
      "          drs_total_len_m    0.012555\n",
      "            Compound_HARD    0.008829\n",
      "          Compound_MEDIUM    0.008103\n",
      "heavy_braking_mean_dv_kmh    0.008012\n",
      "\n",
      "  Computing race-aware permutation importances (uses median MAE per race)...\n",
      "  This may take 3-5 minutes for Random Forest...\n",
      "\n",
      "Race-Aware Permutation Importances (Top 15):\n",
      "            Feature  Importance      Std\n",
      "            LapTime    0.177794 0.006311\n",
      "          TrackTemp    0.019487 0.000379\n",
      "            AirTemp    0.014996 0.000858\n",
      "   avg_corner_angle    0.014658 0.001045\n",
      " straight_len_max_m    0.013761 0.001647\n",
      "           wind_sin    0.010799 0.000561\n",
      "           Humidity    0.010464 0.000845\n",
      "           Pressure    0.007939 0.000825\n",
      "           wind_cos    0.007409 0.000332\n",
      "         LapInStint    0.007043 0.000632\n",
      " LapInStint_squared    0.006744 0.000172\n",
      "          length_km    0.005992 0.000805\n",
      "      Compound_HARD    0.003170 0.000237\n",
      "     straight_ratio    0.003113 0.000229\n",
      "heavy_braking_zones    0.002908 0.000547\n",
      "\n",
      "  (Standard permutation skipped - race-aware version above is more rigorous)\n",
      "\n",
      "✓ RANDOM FOREST COMPLETE - Results saved!\n",
      "  Files created:\n",
      "    - csv_output/rf_results.csv\n",
      "    - csv_output/rf_feature_importances_gini.csv\n",
      "    - csv_output/rf_feature_importances_permutation.csv (overall MAE)\n",
      "    - csv_output/rf_feature_importances_permutation_race.csv (race-aware MAE) ⭐ BEST!\n",
      "    - csv_output/rf_best_hyperparameters.json\n",
      "\n",
      "  Recommendation: Use the race-aware permutation importances for your report!\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 2: RANDOM FOREST (RUN THIS SEPARATELY)\n",
    "# ══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"SECTION 2: RANDOM FOREST MODEL (IMPROVED)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Build pipeline with improved preprocessing\n",
    "preprocessor_rf = ColumnTransformer([\n",
    "    ('num', SimpleImputer(strategy='median'), PHYSICS_FEATURES),\n",
    "    # IMPROVEMENT: drop=None for trees (don't drop reference level)\n",
    "    ('cat', OneHotEncoder(drop=None, handle_unknown='ignore', sparse_output=False), [CATEGORICAL])\n",
    "], remainder='drop')\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    (\"preprocess\", preprocessor_rf),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        random_state=42, \n",
    "        n_jobs=-1,\n",
    "        bootstrap=True,\n",
    "        oob_score=True  # Out-of-bag score for diagnostics\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n✓ Pipeline created (with drop=None + OOB scoring)\")\n",
    "\n",
    "# Enhanced hyperparameter grid\n",
    "# QUICK version: ~48 combinations (manageable on your PC)\n",
    "param_rf = {\n",
    "    \"model__n_estimators\": [300],  # Fixed for speed\n",
    "    \"model__max_depth\": [None, 16, 24],\n",
    "    \"model__min_samples_leaf\": [1, 5, 15],\n",
    "    \"model__min_samples_split\": [2, 10],\n",
    "    \"model__max_features\": [\"sqrt\", 0.4],\n",
    "    \"model__max_samples\": [None, 0.8],  # Bagging subsample\n",
    "}\n",
    "\n",
    "# FULL version (uncomment for exhaustive search - will take MUCH longer!)\n",
    "# param_rf = {\n",
    "#     \"model__n_estimators\": [300, 600],\n",
    "#     \"model__max_depth\": [None, 16, 24],\n",
    "#     \"model__min_samples_leaf\": [1, 5, 15],\n",
    "#     \"model__min_samples_split\": [2, 10, 20],\n",
    "#     \"model__max_features\": [\"sqrt\", \"log2\", 0.4],\n",
    "#     \"model__max_samples\": [None, 0.7, 0.9],\n",
    "# }\n",
    "\n",
    "print(f\"✓ Grid size: {len(list(ParameterGrid(param_rf)))} combinations\")\n",
    "print(f\"  (enhanced with bagging diversity and regularization)\")\n",
    "print(\"  ⚠️  This will still be computationally intensive!\\n\")\n",
    "\n",
    "# Tune\n",
    "print(\"[1/3] Hyperparameter Tuning...\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "best_rf, rf_cv_score, rf_cv_spread = tune_with_race_cv(\n",
    "    pipe_rf, param_rf, X_trainval, y_trainval, race_trainval, n_splits=5\n",
    ")\n",
    "\n",
    "print(f\"\\n  Best hyperparameters:\")\n",
    "for k, v in best_rf.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "print(f\"\\n  CV Performance:\")\n",
    "print(f\"    Median MAE: {rf_cv_score:.4f}s\")\n",
    "print(f\"    Std MAE:    {rf_cv_spread:.4f}s\")\n",
    "\n",
    "# Evaluate on Test\n",
    "print(\"\\n[2/3] Final Evaluation on Test Set...\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "pipe_rf.set_params(**best_rf)\n",
    "\n",
    "# Fit with race-balanced weights\n",
    "w_trainval = race_balanced_weights(race_trainval)\n",
    "pipe_rf.fit(X_trainval, y_trainval, model__sample_weight=w_trainval.values)\n",
    "\n",
    "rf_pred = pipe_rf.predict(X_test)\n",
    "\n",
    "rf_test, rf_perrace = race_median_mae(y_test.values, rf_pred, race_test.values)\n",
    "rf_bystint, rf_stsum = stint_diagnostics(y_test.values, rf_pred, race_test.values, stint_test.values)\n",
    "\n",
    "mae_rf = mean_absolute_error(y_test, rf_pred)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "r2_rf = r2_score(y_test, rf_pred)\n",
    "\n",
    "# Get OOB score (diagnostic)\n",
    "rf_oob = pipe_rf.named_steps['model'].oob_score_\n",
    "\n",
    "print(f\"  PRIMARY METRIC (Race-grouped):\")\n",
    "print(f\"    Median MAE per race: {rf_test:.4f}s\")\n",
    "\n",
    "print(f\"\\n  SECONDARY METRICS (Overall):\")\n",
    "print(f\"    Overall MAE:  {mae_rf:.4f}s\")\n",
    "print(f\"    RMSE:         {rmse_rf:.4f}s\")\n",
    "print(f\"    R²:           {r2_rf:.4f}\")\n",
    "print(f\"    OOB R²:       {rf_oob:.4f} (diagnostic - not race-aware)\")\n",
    "\n",
    "print(f\"\\n  STINT DIAGNOSTICS:\")\n",
    "print(f\"    {rf_stsum.to_string()}\")\n",
    "\n",
    "# Feature importances\n",
    "print(\"\\n[3/3] Feature Importances...\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Gini-based importances (quick but biased)\n",
    "rf_importances_gini, rf_feat_names = extract_feature_importances(pipe_rf, PHYSICS_FEATURES, CATEGORICAL)\n",
    "print(\"\\nGini-based Importances (Top 15):\")\n",
    "print(rf_importances_gini.head(15).to_string(index=False))\n",
    "\n",
    "# Permutation-based importances - RACE-AWARE version (most rigorous!)\n",
    "print(\"\\n  Computing race-aware permutation importances (uses median MAE per race)...\")\n",
    "print(\"  This may take 3-5 minutes for Random Forest...\")\n",
    "rf_importances_perm_race = compute_race_permutation_importance(\n",
    "    pipe_rf, X_val, y_val, race_val, rf_feat_names, n_repeats=10, random_state=42\n",
    ")\n",
    "print(\"\\nRace-Aware Permutation Importances (Top 15):\")\n",
    "print(rf_importances_perm_race.head(15).to_string(index=False))\n",
    "\n",
    "# Also compute standard permutation (for comparison)\n",
    "# NOTE: Commented out to avoid dimension mismatch issues with one-hot encoding\n",
    "# The race-aware version above is what you should use in your report anyway!\n",
    "# If you want standard permutation, use sklearn's permutation_importance directly on the pipeline\n",
    "print(\"\\n  (Standard permutation skipped - race-aware version above is more rigorous)\")\n",
    "rf_importances_perm = rf_importances_perm_race.copy()  # Use race-aware as fallback\n",
    "\n",
    "# Save results\n",
    "rf_results = pd.DataFrame({\n",
    "    \"Model\": [\"Random Forest\"],\n",
    "    \"CV_median_race_MAE\": [rf_cv_score],\n",
    "    \"CV_std\": [rf_cv_spread],\n",
    "    \"TEST_median_race_MAE\": [rf_test],\n",
    "    \"TEST_overall_MAE\": [mae_rf],\n",
    "    \"TEST_RMSE\": [rmse_rf],\n",
    "    \"TEST_R2\": [r2_rf],\n",
    "    \"OOB_R2\": [rf_oob]\n",
    "})\n",
    "\n",
    "rf_results.to_csv('csv_output/rf_results.csv', index=False)\n",
    "rf_importances_gini.to_csv('csv_output/rf_feature_importances_gini.csv', index=False)\n",
    "rf_importances_perm.to_csv('csv_output/rf_feature_importances_permutation.csv', index=False)\n",
    "rf_importances_perm_race.to_csv('csv_output/rf_feature_importances_permutation_race.csv', index=False)\n",
    "\n",
    "with open('csv_output/rf_best_hyperparameters.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'RandomForest': best_rf, \n",
    "        'CV_score': rf_cv_score, \n",
    "        'Test_score': rf_test,\n",
    "        'OOB_R2': rf_oob\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ RANDOM FOREST COMPLETE - Results saved!\")\n",
    "print(\"  Files created:\")\n",
    "print(\"    - csv_output/rf_results.csv\")\n",
    "print(\"    - csv_output/rf_feature_importances_gini.csv\")\n",
    "print(\"    - csv_output/rf_feature_importances_permutation.csv (overall MAE)\")\n",
    "print(\"    - csv_output/rf_feature_importances_permutation_race.csv (race-aware MAE) ⭐ BEST!\")\n",
    "print(\"    - csv_output/rf_best_hyperparameters.json\")\n",
    "print(\"\\n  Recommendation: Use the race-aware permutation importances for your report!\")\n",
    "print(\"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Formula1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
