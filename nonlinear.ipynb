{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0bacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DECISION TREE - Circuit-Disjoint Split (Complete Standalone)\n",
    "\n",
    "Compares Decision Tree against the LINEAR BASELINE:\n",
    "   Tyre + Weather + State (Ridge/Linear)\n",
    "  \n",
    "Uses GroupKFold CV with race-balanced weights.\n",
    "NO TEST LEAKAGE - test set touched only once after tuning.\n",
    "\n",
    "This ensures a fair comparison:\n",
    "  - Same target: LapTime_next_vs_stint_baseline\n",
    "  - Same circuit-disjoint split\n",
    "  - Same feature block (Tyre+Weather+State)\n",
    "  - Different model class only (DT vs Linear)\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GroupKFold, ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49dfe8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "OUTDIR = \"csv_output/nonlinear\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = \"csv_output/Train_set.xlsx\"\n",
    "VAL_PATH = \"csv_output/Validation_set.xlsx\"\n",
    "TEST_PATH = \"csv_output/Test_set.xlsx\"\n",
    "\n",
    "TARGET = \"LapTime_next_vs_stint_baseline\"\n",
    "N_CV_SPLITS = 5\n",
    "\n",
    "NUM_TYRE = [\"is_new_tyre\", \"TyreLife\", \"TyreAgeAtStart\"]\n",
    "NUM_WEATHER = [\"AirTemp\", \"Humidity\", \"Pressure\", \"TrackTemp\", \"wind_sin\", \"wind_cos\"]\n",
    "NUM_STATE = [\n",
    "    \"is_leader\", \"in_drs_range\", \"in_clean_air\", \"in_dirty_air\",\n",
    "    \"pushing\", \"laptime_rolling_std_3\", \"delta_laptime\",\n",
    "    \"cumulative_degradation\", \"LapTime\"\n",
    "]\n",
    "GEOM_COLS_ALL = [\n",
    "    'num_drs_zones', 'length_m', 'num_turns',\n",
    "    'slow_share', 'slow_cluster_max',\n",
    "    'straight_ratio', 'straight_len_max_m', 'n_major_straights',\n",
    "    'heavy_braking_zones', 'heavy_braking_mean_dv_kmh', 'hb_at_end_of_max',\n",
    "    'avg_corner_angle', 'avg_corner_distance', 'drs_total_len_m'\n",
    "]\n",
    "CAT_FEATURES = [\"Compound\", \"RacingNumber\", \"Team\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4d7de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helpers \n",
    "\n",
    "def keep_existing(cols, df):\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "def make_preprocessor(num_feats, cat_feats):\n",
    "    transformers = [(\"num\", SimpleImputer(strategy=\"median\"), num_feats)]\n",
    "    if len(cat_feats) > 0:\n",
    "        try:\n",
    "            ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "        except TypeError:\n",
    "            ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "        transformers.append((\"cat\", ohe, cat_feats))\n",
    "    return ColumnTransformer(transformers, remainder=\"drop\")\n",
    "\n",
    "def compute_race_balanced_weights(race_series):\n",
    "    vc = race_series.value_counts()\n",
    "    return race_series.map(lambda r: 1.0 / vc.loc[r])\n",
    "\n",
    "def tune_with_groupkfold_cv(pipeline, param_grid, X_trainval, y_trainval, race_trainval, n_splits=5):\n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "    gkf = GroupKFold(n_splits=min(n_splits, race_trainval.nunique()))\n",
    "    n_configs = len(list(ParameterGrid(param_grid)))\n",
    "    \n",
    "    for i, params in enumerate(ParameterGrid(param_grid), 1):\n",
    "        fold_scores = []\n",
    "        for tr_idx, va_idx in gkf.split(X_trainval, y_trainval, groups=race_trainval):\n",
    "            race_tr = race_trainval.iloc[tr_idx]\n",
    "            w_tr = compute_race_balanced_weights(race_tr)\n",
    "            pipeline.set_params(**params)\n",
    "            pipeline.fit(X_trainval.iloc[tr_idx], y_trainval.iloc[tr_idx], model__sample_weight=w_tr.values)\n",
    "            yp = pipeline.predict(X_trainval.iloc[va_idx])\n",
    "            race_va = race_trainval.iloc[va_idx]\n",
    "            per_race = []\n",
    "            for rid in race_va.unique():\n",
    "                mask_race = (race_va == rid).values\n",
    "                if mask_race.sum() > 0:\n",
    "                    per_race.append(mean_absolute_error(y_trainval.iloc[va_idx].values[mask_race], yp[mask_race]))\n",
    "            fold_scores.append(float(np.median(per_race)))\n",
    "        \n",
    "        score = float(np.median(fold_scores))\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "        if i % 50 == 0 or i == n_configs:\n",
    "            print(f\"  [{i}/{n_configs}] Best: {best_score:.4f}s\")\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "def refit_and_test_once(pipeline, best_params, X_trainval, y_trainval, race_trainval,\n",
    "                        df_test_sorted, num_feats, cat_feats, target):\n",
    "    w_trainval = compute_race_balanced_weights(race_trainval)\n",
    "    pipeline.set_params(**best_params)\n",
    "    pipeline.fit(X_trainval, y_trainval, model__sample_weight=w_trainval.values)\n",
    "    \n",
    "    X_test = df_test_sorted[num_feats + cat_feats].copy()\n",
    "    y_test = df_test_sorted[target].astype(float).copy()\n",
    "    test_pred_all = pipeline.predict(X_test)\n",
    "    \n",
    "    mask = y_test.notna()\n",
    "    test_pred = test_pred_all[mask]\n",
    "    y_test_valid = y_test[mask]\n",
    "    \n",
    "    test_mae = float(mean_absolute_error(y_test_valid, test_pred))\n",
    "    test_rmse = float(np.sqrt(mean_squared_error(y_test_valid, test_pred)))\n",
    "    test_r2 = float(r2_score(y_test_valid, test_pred))\n",
    "    \n",
    "    per_race_mae = []\n",
    "    race_ids = []\n",
    "    for (year, round_no, circuit), g in df_test_sorted.groupby(['year', 'round', 'name'], sort=False):\n",
    "        group_indices = g.index.values\n",
    "        group_mask = np.zeros(len(df_test_sorted), dtype=bool)\n",
    "        group_mask[group_indices] = True\n",
    "        group_mask = group_mask & mask.values\n",
    "        if group_mask.sum() > 0:\n",
    "            race_mae = mean_absolute_error(df_test_sorted[target].values[group_mask], test_pred_all[group_mask])\n",
    "            per_race_mae.append(race_mae)\n",
    "            race_ids.append(f\"{year}_{round_no}_{circuit}\")\n",
    "    \n",
    "    median_mae_per_race = float(np.median(per_race_mae))\n",
    "    \n",
    "    return pipeline, {\n",
    "        \"MAE\": test_mae,\n",
    "        \"RMSE\": test_rmse,\n",
    "        \"R2\": test_r2,\n",
    "        \"median_MAE_per_race\": median_mae_per_race,\n",
    "        \"per_race_mae\": per_race_mae,\n",
    "        \"race_ids\": race_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac37e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "DECISION TREE - Baseline vs Baseline+Geometry\n",
      "====================================================================================================\n",
      "\n",
      "Loading data...\n",
      "Train: 26,221 | Val: 7,199 | Test: 6,976\n",
      "✓ Verified: no circuit overlap across Train/Val/Test\n",
      "\n",
      "Feature sets:\n",
      "  Baseline (Tyre+Weather+State): 18 features\n",
      "  +Geometry: 29 features (+11)\n",
      "\n",
      "Train+Val: 33,420 samples, 19 unique circuits\n",
      "Hyperparameter grid: 16 configs\n",
      "\n",
      "====================================================================================================\n",
      "Tyre+Weather+State\n",
      "====================================================================================================\n",
      "Samples: 33,420\n",
      "Unique circuits (CV groups): 19\n",
      "Tuning (33,420 train samples)...\n",
      "  [16/16] Best: 0.3852s\n",
      "\n",
      "Results:\n",
      "  CV MAE:               0.3852s\n",
      "  Test MAE:             0.3601s\n",
      "  Test RMSE:            0.5647s\n",
      "  Test R²:              0.1055\n",
      "  Test Median MAE/race: 0.3409s\n",
      "  Tree depth:           10\n",
      "  Tree leaves:          166\n",
      "Saved: csv_output/nonlinear/dt_Tyre_Weather_State_config.json\n",
      "====================================================================================================\n",
      "Tyre+Weather+State+Geometry\n",
      "====================================================================================================\n",
      "Samples: 33,420\n",
      "Unique circuits (CV groups): 19\n",
      "Tuning (33,420 train samples)...\n",
      "  [16/16] Best: 0.3839s\n",
      "\n",
      "Results:\n",
      "  CV MAE:               0.3839s\n",
      "  Test MAE:             0.3876s\n",
      "  Test RMSE:            0.6119s\n",
      "  Test R²:              -0.0502\n",
      "  Test Median MAE/race: 0.3361s\n",
      "  Tree depth:           10\n",
      "  Tree leaves:          190\n",
      "Saved: csv_output/nonlinear/dt_Tyre_Weather_State_Geometry_config.json\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"DECISION TREE - Baseline vs Baseline+Geometry\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading data...\")\n",
    "df_train = pd.read_excel(TRAIN_PATH)\n",
    "df_val = pd.read_excel(VAL_PATH)\n",
    "df_test = pd.read_excel(TEST_PATH)\n",
    "print(f\"Train: {len(df_train):,} | Val: {len(df_val):,} | Test: {len(df_test):,}\")\n",
    "\n",
    "assert TARGET in df_train.columns, f\"Target '{TARGET}' not found!\"\n",
    "\n",
    "# SANITY CHECK: Verify circuit-disjoint splits (no circuit overlap)\n",
    "train_c = set(df_train[\"name\"].unique())\n",
    "val_c   = set(df_val[\"name\"].unique())\n",
    "test_c  = set(df_test[\"name\"].unique())\n",
    "assert train_c.isdisjoint(val_c) and train_c.isdisjoint(test_c) and val_c.isdisjoint(test_c), \"ERROR: Circuit overlap in splits!\"\n",
    "print(\"✓ Verified: no circuit overlap across Train/Val/Test\")\n",
    "\n",
    "# Ensure is_new_tyre\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    if 'is_new_tyre' not in df.columns and 'TyreAgeAtStart' in df.columns:\n",
    "        df['is_new_tyre'] = (df['TyreAgeAtStart'] == 0).astype(int)\n",
    "\n",
    "# Create race_id (for per-race reporting) and circuit_id (for unseen-circuit CV grouping)\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df['race_id'] = (\n",
    "        df['year'].astype(str) + '_' + \n",
    "        df['round'].astype(str) + '_' + \n",
    "        df['name'].astype(str)\n",
    "    )\n",
    "    df['circuit_id'] = df['name'].astype(str)  # For unseen-circuit CV grouping\n",
    "\n",
    "# Prepare features\n",
    "num_tyre = keep_existing(NUM_TYRE, df_train)\n",
    "num_weather = keep_existing(NUM_WEATHER, df_train)\n",
    "num_state = keep_existing(NUM_STATE, df_train)\n",
    "num_geometry = [c for c in GEOM_COLS_ALL if c in df_train.columns and c != \"hb_at_end_of_max\"]\n",
    "CAT = keep_existing(CAT_FEATURES, df_train)\n",
    "\n",
    "feat_baseline = num_tyre + num_weather + num_state\n",
    "feat_with_geom = num_tyre + num_weather + num_state + num_geometry\n",
    "\n",
    "print(f\"\\nFeature sets:\")\n",
    "print(f\"  Baseline (Tyre+Weather+State): {len(feat_baseline)} features\")\n",
    "print(f\"  +Geometry: {len(feat_with_geom)} features (+{len(feat_with_geom)-len(feat_baseline)})\")\n",
    "\n",
    "# Combine train+val\n",
    "df_trainval = pd.concat([df_train, df_val], axis=0, ignore_index=True)\n",
    "print(f\"\\nTrain+Val: {len(df_trainval):,} samples, {df_trainval['circuit_id'].nunique()} unique circuits\")\n",
    "\n",
    "# Sort test\n",
    "sort_cols = ['year', 'round', 'name']\n",
    "if 'lap_number' in df_test.columns:\n",
    "    sort_cols.append('lap_number')\n",
    "elif 'LapNumber' in df_test.columns:\n",
    "    sort_cols.append('LapNumber')\n",
    "df_test_sorted = df_test.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_dt = {\n",
    "    \"model__max_depth\": [6, 10],\n",
    "    \"model__min_samples_leaf\": [5, 10],\n",
    "    \"model__max_features\": [\"sqrt\", 0.5],\n",
    "    \"model__ccp_alpha\": [0.0, 1e-4],\n",
    "}\n",
    "\n",
    "# For more extensive search (commented out as in original):\n",
    "# param_dt = {\n",
    "#     \"model__max_depth\": [4, 6, 8, 12],\n",
    "#     \"model__min_samples_leaf\": [5, 10, 20],\n",
    "#     \"model__min_samples_split\": [2, 10],\n",
    "#     \"model__max_features\": [None, \"sqrt\", 0.5],\n",
    "#     \"model__max_leaf_nodes\": [None, 64, 128, 256],\n",
    "#     \"model__ccp_alpha\": [0.0, 1e-4, 5e-4],\n",
    "# }\n",
    "\n",
    "\n",
    "n_configs = len(list(ParameterGrid(param_dt)))\n",
    "print(f\"Hyperparameter grid: {n_configs} configs\\n\")\n",
    "\n",
    "# =========================================================\n",
    "# RUN BOTH FEATURE SETS\n",
    "# =========================================================\n",
    "results = []\n",
    "\n",
    "for set_name, num_feats in [(\"Tyre+Weather+State\", feat_baseline), \n",
    "                             (\"Tyre+Weather+State+Geometry\", feat_with_geom)]:\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{set_name}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    X_trainval = df_trainval[num_feats + CAT].copy()\n",
    "    y_trainval = df_trainval[TARGET].astype(float).copy()\n",
    "    circuit_trainval = df_trainval['circuit_id'].copy()  # Unseen-circuit grouping\n",
    "    race_trainval = df_trainval['race_id'].copy()        # For per-race reporting\n",
    "    \n",
    "    mask_trainval = y_trainval.notna()\n",
    "    X_trainval = X_trainval.loc[mask_trainval]\n",
    "    y_trainval = y_trainval.loc[mask_trainval]\n",
    "    circuit_trainval = circuit_trainval.loc[mask_trainval]\n",
    "    race_trainval = race_trainval.loc[mask_trainval]\n",
    "    \n",
    "    print(f\"Samples: {len(X_trainval):,}\")\n",
    "    print(f\"Unique circuits (CV groups): {circuit_trainval.nunique()}\")\n",
    "    \n",
    "    preprocessor = make_preprocessor(num_feats, CAT)\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", DecisionTreeRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    print(f\"Tuning ({len(X_trainval):,} train samples)...\")\n",
    "    best_params, best_cv_score = tune_with_groupkfold_cv(\n",
    "        pipeline=pipeline,\n",
    "        param_grid=param_dt,\n",
    "        X_trainval=X_trainval,\n",
    "        y_trainval=y_trainval,\n",
    "        race_trainval=circuit_trainval,  # Group by CIRCUIT for unseen-circuit CV\n",
    "        n_splits=N_CV_SPLITS\n",
    "    )\n",
    "    \n",
    "    pipeline_fitted, test_results = refit_and_test_once(\n",
    "        pipeline=pipeline,\n",
    "        best_params=best_params,\n",
    "        X_trainval=X_trainval,\n",
    "        y_trainval=y_trainval,\n",
    "        race_trainval=circuit_trainval,  # Weight by CIRCUIT\n",
    "        df_test_sorted=df_test_sorted,\n",
    "        num_feats=num_feats,\n",
    "        cat_feats=CAT,\n",
    "        target=TARGET\n",
    "    )\n",
    "    \n",
    "    tree_depth = pipeline_fitted.named_steps['model'].get_depth()\n",
    "    tree_leaves = pipeline_fitted.named_steps['model'].get_n_leaves()\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  CV MAE:               {best_cv_score:.4f}s\")\n",
    "    print(f\"  Test MAE:             {test_results['MAE']:.4f}s\")\n",
    "    print(f\"  Test RMSE:            {test_results['RMSE']:.4f}s\")\n",
    "    print(f\"  Test R²:              {test_results['R2']:.4f}\")\n",
    "    print(f\"  Test Median MAE/race: {test_results['median_MAE_per_race']:.4f}s\")\n",
    "    print(f\"  Tree depth:           {tree_depth}\")\n",
    "    print(f\"  Tree leaves:          {tree_leaves}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"block\": set_name,\n",
    "        \"n_features\": len(num_feats) + len(CAT),\n",
    "        \"CV_MAE_per_race\": best_cv_score,\n",
    "        \"test_MAE\": test_results['MAE'],\n",
    "        \"test_RMSE\": test_results['RMSE'],\n",
    "        \"test_R2\": test_results['R2'],\n",
    "        \"test_median_MAE_per_race\": test_results['median_MAE_per_race'],\n",
    "        \"tree_depth\": tree_depth,\n",
    "        \"tree_leaves\": tree_leaves,\n",
    "        \"best_params\": json.dumps(best_params),\n",
    "    })\n",
    "    \n",
    "    # Save per-race MAE\n",
    "    per_race_df = pd.DataFrame({\n",
    "        'Race': test_results['race_ids'],\n",
    "        'MAE': test_results['per_race_mae']\n",
    "    }).sort_values('MAE', ascending=False)\n",
    "    per_race_path = os.path.join(OUTDIR, f\"dt_{set_name.replace('+', '_')}_per_race_mae.csv\")\n",
    "    per_race_df.to_csv(per_race_path, index=False)\n",
    "    \n",
    "    # Save model config as JSON\n",
    "    model_info = {\n",
    "        \"model\": \"DecisionTree\",\n",
    "        \"feature_set\": set_name,\n",
    "        \"n_features\": len(num_feats) + len(CAT),\n",
    "        \"best_params\": best_params,\n",
    "        \"cv_mae_median_per_circuit\": float(best_cv_score),\n",
    "        \"test_mae\": float(test_results['MAE']),\n",
    "        \"test_rmse\": float(test_results['RMSE']),\n",
    "        \"test_r2\": float(test_results['R2']),\n",
    "        \"test_median_mae_per_race\": float(test_results['median_MAE_per_race']),\n",
    "        \"tree_depth\": int(tree_depth),\n",
    "        \"tree_leaves\": int(tree_leaves),\n",
    "    }\n",
    "    model_json_path = os.path.join(OUTDIR, f\"dt_{set_name.replace('+', '_')}_config.json\")\n",
    "    with open(model_json_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(f\"Saved: {model_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87617666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "                      block  test_MAE  test_RMSE   test_R2  tree_depth  tree_leaves\n",
      "         Tyre+Weather+State  0.345124   0.516537  0.251697           6           27\n",
      "Tyre+Weather+State+Geometry  0.483366   0.791382 -0.756493           9           50\n",
      "\n",
      "Baseline MAE:      0.3451s\n",
      "+Geometry MAE:     0.4834s\n",
      "Difference:        +0.1382s (+40.1%)\n",
      "→ Geometry HURTS by 40.1%\n",
      "\n",
      "✓ Saved to csv_output/nonlinear/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================================================\n",
    "# SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "summary = pd.DataFrame(results)\n",
    "summary_path = os.path.join(OUTDIR, \"dt_comparison.csv\")\n",
    "summary.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\n\" + summary[['block', 'test_MAE', 'test_RMSE', 'test_R2', 'tree_depth', 'tree_leaves']].to_string(index=False))\n",
    "\n",
    "# Difference\n",
    "baseline = summary.iloc[0]\n",
    "with_geom = summary.iloc[1]\n",
    "\n",
    "diff_mae = with_geom['test_MAE'] - baseline['test_MAE']\n",
    "pct_change = (diff_mae / baseline['test_MAE']) * 100\n",
    "\n",
    "print(f\"\\nBaseline MAE:      {baseline['test_MAE']:.4f}s\")\n",
    "print(f\"+Geometry MAE:     {with_geom['test_MAE']:.4f}s\")\n",
    "print(f\"Difference:        {diff_mae:+.4f}s ({pct_change:+.1f}%)\")\n",
    "\n",
    "if abs(diff_mae) < 0.0001:\n",
    "    print(\"→ Same (geometry neutral)\")\n",
    "elif diff_mae < 0:\n",
    "    print(f\"→ Geometry HELPS by {abs(pct_change):.1f}%\")\n",
    "else:\n",
    "    print(f\"→ Geometry HURTS by {pct_change:.1f}%\")\n",
    "\n",
    "print(f\"\\n✓ Saved to {OUTDIR}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d991df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "RANDOM FOREST - Baseline vs Baseline+Geometry\n",
      "====================================================================================================\n",
      "\n",
      "Hyperparameter grid - Random Forest: 432 configs\n",
      "\n",
      "\n",
      "Tyre+Weather+State\n",
      "Tuning (33,420 train samples)...\n",
      "  [50/432] Best: 0.3639s\n",
      "  [100/432] Best: 0.3520s\n",
      "  [150/432] Best: 0.3520s\n",
      "  [200/432] Best: 0.3520s\n",
      "  [250/432] Best: 0.3518s\n",
      "  [300/432] Best: 0.3518s\n",
      "  [350/432] Best: 0.3518s\n",
      "  [400/432] Best: 0.3518s\n",
      "  [432/432] Best: 0.3518s\n",
      "\n",
      "Results:\n",
      "  CV MAE (median per-circuit): 0.3518s\n",
      "  Test MAE:                    0.3348s\n",
      "  Test RMSE:                   0.4859s\n",
      "  Test R²:                     0.3378\n",
      "  Test Median MAE/race:        0.3012s\n",
      "  N estimators:                600\n",
      "Saved: csv_output/nonlinear/rf_Tyre_Weather_State_config.json\n",
      "\n",
      "Tyre+Weather+State+Geometry\n",
      "Tuning (33,420 train samples)...\n",
      "  [50/432] Best: 0.3579s\n",
      "  [100/432] Best: 0.3479s\n",
      "  [150/432] Best: 0.3475s\n",
      "  [200/432] Best: 0.3475s\n",
      "  [250/432] Best: 0.3475s\n",
      "  [300/432] Best: 0.3475s\n",
      "  [350/432] Best: 0.3474s\n",
      "  [400/432] Best: 0.3474s\n",
      "  [432/432] Best: 0.3474s\n",
      "\n",
      "Results:\n",
      "  CV MAE (median per-circuit): 0.3474s\n",
      "  Test MAE:                    0.3334s\n",
      "  Test RMSE:                   0.4842s\n",
      "  Test R²:                     0.3424\n",
      "  Test Median MAE/race:        0.3010s\n",
      "  N estimators:                600\n",
      "Saved: csv_output/nonlinear/rf_Tyre_Weather_State_Geometry_config.json\n",
      "\n",
      "====================================================================================================\n",
      "RANDOM FOREST - COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "                      block  test_MAE  test_RMSE  test_R2  n_estimators\n",
      "         Tyre+Weather+State  0.334849   0.485914 0.337795           600\n",
      "Tyre+Weather+State+Geometry  0.333403   0.484214 0.342419           600\n",
      "\n",
      "Baseline MAE:      0.3348s\n",
      "+Geometry MAE:     0.3334s\n",
      "Difference:        -0.0014s (-0.4%)\n",
      "→ Geometry: HELPS by 0.4%\n",
      "\n",
      "✓ All results saved to csv_output/nonlinear/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================================================\n",
    "# RANDOM FOREST\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RANDOM FOREST - Baseline vs Baseline+Geometry\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Random Forest hyperparameter grid\n",
    "# param_rf = {\n",
    "#     \"model__n_estimators\": [300, 600],\n",
    "#     \"model__max_depth\": [8, 12, None],\n",
    "#     \"model__min_samples_leaf\": [1, 5, 20],\n",
    "#     \"model__max_features\": [\"sqrt\", 0.5],\n",
    "# }\n",
    "\n",
    "#More extensive grid (commented):\n",
    "param_rf = {\n",
    "    \"model__n_estimators\": [100, 300, 600],\n",
    "    \"model__max_depth\": [8, 12, 16, None],\n",
    "    \"model__min_samples_leaf\": [1, 5, 10, 20],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__max_features\": [\"sqrt\", 0.5, 0.7],\n",
    "}\n",
    "\n",
    "n_configs_rf = len(list(ParameterGrid(param_rf)))\n",
    "print(f\"\\nHyperparameter grid - Random Forest: {n_configs_rf} configs\\n\")\n",
    "\n",
    "rf_results = []\n",
    "\n",
    "for set_name, num_feats in [(\"Tyre+Weather+State\", feat_baseline), \n",
    "                             (\"Tyre+Weather+State+Geometry\", feat_with_geom)]:\n",
    "    print(f\"\\n{set_name}\")\n",
    "    \n",
    "    X_trainval = df_trainval[num_feats + CAT].copy()\n",
    "    y_trainval = df_trainval[TARGET].astype(float).copy()\n",
    "    circuit_trainval = df_trainval['circuit_id'].copy()  # Unseen-circuit grouping\n",
    "    \n",
    "    mask_trainval = y_trainval.notna()\n",
    "    X_trainval = X_trainval.loc[mask_trainval]\n",
    "    y_trainval = y_trainval.loc[mask_trainval]\n",
    "    circuit_trainval = circuit_trainval.loc[mask_trainval]\n",
    "    \n",
    "    preprocessor = make_preprocessor(num_feats, CAT)\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    print(f\"Tuning ({len(X_trainval):,} train samples)...\")\n",
    "    best_params, best_cv_score = tune_with_groupkfold_cv(\n",
    "        pipeline=pipeline,\n",
    "        param_grid=param_rf,\n",
    "        X_trainval=X_trainval,\n",
    "        y_trainval=y_trainval,\n",
    "        race_trainval=circuit_trainval,  # Group by CIRCUIT for unseen-circuit CV\n",
    "        n_splits=N_CV_SPLITS\n",
    "    )\n",
    "    \n",
    "    pipeline_fitted, test_results = refit_and_test_once(\n",
    "        pipeline=pipeline,\n",
    "        best_params=best_params,\n",
    "        X_trainval=X_trainval,\n",
    "        y_trainval=y_trainval,\n",
    "        race_trainval=circuit_trainval,  # Weight by CIRCUIT\n",
    "        df_test_sorted=df_test_sorted,\n",
    "        num_feats=num_feats,\n",
    "        cat_feats=CAT,\n",
    "        target=TARGET\n",
    "    )\n",
    "    \n",
    "    n_estimators = pipeline_fitted.named_steps['model'].n_estimators\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  CV MAE (median per-circuit): {best_cv_score:.4f}s\")\n",
    "    print(f\"  Test MAE:                    {test_results['MAE']:.4f}s\")\n",
    "    print(f\"  Test RMSE:                   {test_results['RMSE']:.4f}s\")\n",
    "    print(f\"  Test R²:                     {test_results['R2']:.4f}\")\n",
    "    print(f\"  Test Median MAE/race:        {test_results['median_MAE_per_race']:.4f}s\")\n",
    "    print(f\"  N estimators:                {n_estimators}\")\n",
    "    \n",
    "    rf_results.append({\n",
    "        \"block\": set_name,\n",
    "        \"n_features\": len(num_feats) + len(CAT),\n",
    "        \"CV_MAE_per_circuit\": best_cv_score,\n",
    "        \"test_MAE\": test_results['MAE'],\n",
    "        \"test_RMSE\": test_results['RMSE'],\n",
    "        \"test_R2\": test_results['R2'],\n",
    "        \"test_median_MAE_per_race\": test_results['median_MAE_per_race'],\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"best_params\": json.dumps(best_params),\n",
    "    })\n",
    "    \n",
    "    # Save per-race MAE\n",
    "    per_race_df = pd.DataFrame({\n",
    "        'Race': test_results['race_ids'],\n",
    "        'MAE': test_results['per_race_mae']\n",
    "    }).sort_values('MAE', ascending=False)\n",
    "    per_race_path = os.path.join(OUTDIR, f\"rf_{set_name.replace('+', '_')}_per_race_mae.csv\")\n",
    "    per_race_df.to_csv(per_race_path, index=False)\n",
    "    \n",
    "    # Save model config as JSON\n",
    "    model_info = {\n",
    "        \"model\": \"RandomForest\",\n",
    "        \"feature_set\": set_name,\n",
    "        \"n_features\": len(num_feats) + len(CAT),\n",
    "        \"best_params\": best_params,\n",
    "        \"cv_mae_median_per_circuit\": float(best_cv_score),\n",
    "        \"test_mae\": float(test_results['MAE']),\n",
    "        \"test_rmse\": float(test_results['RMSE']),\n",
    "        \"test_r2\": float(test_results['R2']),\n",
    "        \"test_median_mae_per_race\": float(test_results['median_MAE_per_race']),\n",
    "        \"n_estimators\": int(n_estimators),\n",
    "    }\n",
    "    model_json_path = os.path.join(OUTDIR, f\"rf_{set_name.replace('+', '_')}_config.json\")\n",
    "    with open(model_json_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(f\"Saved: {model_json_path}\")\n",
    "\n",
    "# =========================================================\n",
    "# RANDOM FOREST SUMMARY\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RANDOM FOREST - COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "rf_summary = pd.DataFrame(rf_results)\n",
    "rf_summary_path = os.path.join(OUTDIR, \"rf_comparison.csv\")\n",
    "rf_summary.to_csv(rf_summary_path, index=False)\n",
    "\n",
    "print(\"\\n\" + rf_summary[['block', 'test_MAE', 'test_RMSE', 'test_R2', 'n_estimators']].to_string(index=False))\n",
    "\n",
    "baseline_rf = rf_summary.iloc[0]\n",
    "with_geom_rf = rf_summary.iloc[1]\n",
    "diff_mae_rf = with_geom_rf['test_MAE'] - baseline_rf['test_MAE']\n",
    "pct_change_rf = (diff_mae_rf / baseline_rf['test_MAE']) * 100\n",
    "\n",
    "print(f\"\\nBaseline MAE:      {baseline_rf['test_MAE']:.4f}s\")\n",
    "print(f\"+Geometry MAE:     {with_geom_rf['test_MAE']:.4f}s\")\n",
    "print(f\"Difference:        {diff_mae_rf:+.4f}s ({pct_change_rf:+.1f}%)\")\n",
    "\n",
    "if abs(diff_mae_rf) < 0.0001:\n",
    "    print(\"→ Geometry: NEUTRAL\")\n",
    "elif diff_mae_rf < 0:\n",
    "    print(f\"→ Geometry: HELPS by {abs(pct_change_rf):.1f}%\")\n",
    "else:\n",
    "    print(f\"→ Geometry: HURTS by {pct_change_rf:.1f}%\")\n",
    "\n",
    "print(f\"\\n✓ All results saved to {OUTDIR}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84701ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "HIST GRADIENT BOOSTING - Circuit-Disjoint Evaluation (GroupKFold CV)\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Hyperparameter grid - HistGradientBoosting: 576 configs\n",
      "\n",
      "\n",
      "Tyre+Weather+State\n",
      "Tuning (33,420 train samples)...\n",
      "  Running GroupKFold CV (5 splits, 576 configs, grouped by CIRCUIT)...\n",
      "    [50/576] Best CV: 0.3621s\n",
      "    [100/576] Best CV: 0.3618s\n",
      "    [150/576] Best CV: 0.3618s\n",
      "    [200/576] Best CV: 0.3618s\n",
      "    [250/576] Best CV: 0.3616s\n",
      "    [300/576] Best CV: 0.3616s\n",
      "    [350/576] Best CV: 0.3616s\n",
      "    [400/576] Best CV: 0.3616s\n",
      "    [450/576] Best CV: 0.3616s\n",
      "    [500/576] Best CV: 0.3616s\n",
      "    [550/576] Best CV: 0.3616s\n",
      "    [576/576] Best CV: 0.3616s\n",
      "  ✓ Best CV score: 0.3616s\n",
      "\n",
      "Results:\n",
      "  CV MAE (median per-circuit): 0.3616s\n",
      "  Test MAE:                    0.3348s\n",
      "  Test RMSE:                   0.4989s\n",
      "  Test R²:                     0.3020\n",
      "  Test Median MAE/circuit:     0.3182s\n",
      "  N iterations:                100\n",
      "Saved: csv_output/nonlinear/hgb_Tyre_Weather_State_config.json\n",
      "\n",
      "Tyre+Weather+State+Geometry\n",
      "Tuning (33,420 train samples)...\n",
      "  Running GroupKFold CV (5 splits, 576 configs, grouped by CIRCUIT)...\n",
      "    [50/576] Best CV: 0.3596s\n",
      "    [100/576] Best CV: 0.3596s\n",
      "    [150/576] Best CV: 0.3596s\n",
      "    [200/576] Best CV: 0.3596s\n",
      "    [250/576] Best CV: 0.3594s\n",
      "    [300/576] Best CV: 0.3594s\n",
      "    [350/576] Best CV: 0.3594s\n",
      "    [400/576] Best CV: 0.3594s\n",
      "    [450/576] Best CV: 0.3594s\n",
      "    [500/576] Best CV: 0.3594s\n",
      "    [550/576] Best CV: 0.3594s\n",
      "    [576/576] Best CV: 0.3594s\n",
      "  ✓ Best CV score: 0.3594s\n",
      "\n",
      "Results:\n",
      "  CV MAE (median per-circuit): 0.3594s\n",
      "  Test MAE:                    0.3419s\n",
      "  Test RMSE:                   0.5068s\n",
      "  Test R²:                     0.2796\n",
      "  Test Median MAE/circuit:     0.3041s\n",
      "  N iterations:                1000\n",
      "Saved: csv_output/nonlinear/hgb_Tyre_Weather_State_Geometry_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# HIST GRADIENT BOOSTING - Circuit-Disjoint Split\n",
    "\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"HIST GRADIENT BOOSTING - Circuit-Disjoint Evaluation (GroupKFold CV)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "# Helper function (HGB-specific, no sample_weight)\n",
    "\n",
    "def tune_hgb_with_groupkfold_cv(pipeline, param_grid, X_trainval, y_trainval, circuit_trainval, \n",
    "                                 n_splits=5):\n",
    "    \"\"\"\n",
    "    GroupKFold CV for HistGradientBoosting (NO sample_weight - not supported).\n",
    "    Returns best params based on median MAE per circuit across folds.\n",
    "    Groups by CIRCUIT for unseen-circuit evaluation.\n",
    "    \"\"\"\n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=min(n_splits, circuit_trainval.nunique()))\n",
    "    n_configs = len(list(ParameterGrid(param_grid)))\n",
    "    \n",
    "    print(f\"  Running GroupKFold CV ({n_splits} splits, {n_configs} configs, grouped by CIRCUIT)...\")\n",
    "    \n",
    "    for i, params in enumerate(ParameterGrid(param_grid), 1):\n",
    "        fold_scores = []\n",
    "        \n",
    "        for tr_idx, va_idx in gkf.split(X_trainval, y_trainval, groups=circuit_trainval):\n",
    "            # Fit WITHOUT sample weights (HGB doesn't support them reliably)\n",
    "            pipeline.set_params(**params)\n",
    "            pipeline.fit(X_trainval.iloc[tr_idx], y_trainval.iloc[tr_idx])\n",
    "            \n",
    "            # Predict on validation fold\n",
    "            yp = pipeline.predict(X_trainval.iloc[va_idx])\n",
    "            \n",
    "            # Compute per-circuit MAE for this fold\n",
    "            circuit_va = circuit_trainval.iloc[va_idx]\n",
    "            per_circuit = []\n",
    "            for cid in circuit_va.unique():\n",
    "                mask_circuit = (circuit_va == cid).values\n",
    "                if mask_circuit.sum() > 0:\n",
    "                    per_circuit.append(mean_absolute_error(\n",
    "                        y_trainval.iloc[va_idx].values[mask_circuit], \n",
    "                        yp[mask_circuit]\n",
    "                    ))\n",
    "            \n",
    "            fold_scores.append(float(np.median(per_circuit)))\n",
    "        \n",
    "        # Median across folds\n",
    "        score = float(np.median(fold_scores))\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "        \n",
    "        if i % 50 == 0 or i == n_configs:\n",
    "            print(f\"    [{i}/{n_configs}] Best CV: {best_score:.4f}s\")\n",
    "    \n",
    "    print(f\"  ✓ Best CV score: {best_score:.4f}s\")\n",
    "    return best_params, best_score\n",
    "\n",
    "# =========================================================\n",
    "# HYPERPARAMETER GRID FOR HGB\n",
    "# =========================================================\n",
    "# param_hgb = {\n",
    "#     \"model__max_depth\": [3, 5, None],\n",
    "#     \"model__learning_rate\": [0.05, 0.1],\n",
    "#     \"model__max_iter\": [300, 600],\n",
    "#     \"model__min_samples_leaf\": [20, 50],\n",
    "# }\n",
    "\n",
    "#More extensive grid (commented):\n",
    "param_hgb = {\n",
    "    \"model__max_depth\": [3, 5, 7, None],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"model__max_iter\": [100, 300, 600, 1000],\n",
    "    \"model__min_samples_leaf\": [20, 50, 100],\n",
    "    \"model__l2_regularization\": [0.0, 0.1, 1.0],\n",
    "}\n",
    "\n",
    "\n",
    "n_configs_hgb = len(list(ParameterGrid(param_hgb)))\n",
    "print(f\"\\n\\nHyperparameter grid - HistGradientBoosting: {n_configs_hgb} configs\\n\")\n",
    "\n",
    "\n",
    "# RUN HGB EXPERIMENTS (using same feature sets as DT/RF)\n",
    "\n",
    "hgb_results = []\n",
    "\n",
    "for set_name, num_feats in [(\"Tyre+Weather+State\", feat_baseline), \n",
    "                             (\"Tyre+Weather+State+Geometry\", feat_with_geom)]:\n",
    "    print(f\"\\n{set_name}\")\n",
    "    \n",
    "    X_trainval = df_trainval[num_feats + CAT].copy()\n",
    "    y_trainval = df_trainval[TARGET].astype(float).copy()\n",
    "    circuit_trainval = df_trainval['circuit_id'].copy()\n",
    "    \n",
    "    mask_trainval = y_trainval.notna()\n",
    "    X_trainval = X_trainval.loc[mask_trainval]\n",
    "    y_trainval = y_trainval.loc[mask_trainval]\n",
    "    circuit_trainval = circuit_trainval.loc[mask_trainval]\n",
    "    \n",
    "    preprocessor = make_preprocessor(num_feats, CAT)\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", HistGradientBoostingRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    print(f\"Tuning ({len(X_trainval):,} train samples)...\")\n",
    "    best_params, best_cv_score = tune_hgb_with_groupkfold_cv(\n",
    "        pipeline=pipeline,\n",
    "        param_grid=param_hgb,\n",
    "        X_trainval=X_trainval,\n",
    "        y_trainval=y_trainval,\n",
    "        circuit_trainval=circuit_trainval,\n",
    "        n_splits=N_CV_SPLITS\n",
    "    )\n",
    "    \n",
    "    # Refit and test\n",
    "    pipeline.set_params(**best_params)\n",
    "    pipeline.fit(X_trainval, y_trainval)\n",
    "    \n",
    "    X_test = df_test_sorted[num_feats + CAT].copy()\n",
    "    y_test = df_test_sorted[TARGET].astype(float).copy()\n",
    "    test_pred_all = pipeline.predict(X_test)\n",
    "    \n",
    "    mask = y_test.notna()\n",
    "    test_pred = test_pred_all[mask]\n",
    "    y_test_valid = y_test[mask]\n",
    "    \n",
    "    test_mae = float(mean_absolute_error(y_test_valid, test_pred))\n",
    "    test_rmse = float(np.sqrt(mean_squared_error(y_test_valid, test_pred)))\n",
    "    test_r2 = float(r2_score(y_test_valid, test_pred))\n",
    "    \n",
    "    # Per-circuit MAE\n",
    "    per_circuit_mae = []\n",
    "    circuit_names = []\n",
    "    for circuit in df_test_sorted['name'].unique():\n",
    "        circuit_mask = (df_test_sorted['name'] == circuit).values\n",
    "        circuit_mask = circuit_mask & mask.values\n",
    "        if circuit_mask.sum() > 0:\n",
    "            circuit_mae = mean_absolute_error(\n",
    "                df_test_sorted[TARGET].values[circuit_mask], \n",
    "                test_pred_all[circuit_mask]\n",
    "            )\n",
    "            per_circuit_mae.append(circuit_mae)\n",
    "            circuit_names.append(str(circuit))\n",
    "    \n",
    "    median_mae_per_circuit = float(np.median(per_circuit_mae))\n",
    "    n_iter = pipeline.named_steps['model'].n_iter_\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  CV MAE (median per-circuit): {best_cv_score:.4f}s\")\n",
    "    print(f\"  Test MAE:                    {test_mae:.4f}s\")\n",
    "    print(f\"  Test RMSE:                   {test_rmse:.4f}s\")\n",
    "    print(f\"  Test R²:                     {test_r2:.4f}\")\n",
    "    print(f\"  Test Median MAE/circuit:     {median_mae_per_circuit:.4f}s\")\n",
    "    print(f\"  N iterations:                {n_iter}\")\n",
    "    \n",
    "    hgb_results.append({\n",
    "        \"block\": set_name,\n",
    "        \"n_features\": len(num_feats) + len(CAT),\n",
    "        \"CV_MAE_per_circuit\": best_cv_score,\n",
    "        \"test_MAE\": test_mae,\n",
    "        \"test_RMSE\": test_rmse,\n",
    "        \"test_R2\": test_r2,\n",
    "        \"test_median_MAE_per_circuit\": median_mae_per_circuit,\n",
    "        \"n_iter\": n_iter,\n",
    "        \"best_params\": json.dumps(best_params),\n",
    "    })\n",
    "    \n",
    "    # Save per-circuit MAE\n",
    "    per_circuit_df = pd.DataFrame({\n",
    "        'Circuit': circuit_names,\n",
    "        'MAE': per_circuit_mae\n",
    "    }).sort_values('MAE', ascending=False)\n",
    "    per_circuit_path = os.path.join(OUTDIR, f\"hgb_{set_name.replace('+', '_')}_per_circuit_mae.csv\")\n",
    "    per_circuit_df.to_csv(per_circuit_path, index=False)\n",
    "    \n",
    "    # Save model config as JSON\n",
    "    model_info = {\n",
    "        \"model\": \"HistGradientBoosting\",\n",
    "        \"feature_set\": set_name,\n",
    "        \"n_features\": len(num_feats) + len(CAT),\n",
    "        \"best_params\": best_params,\n",
    "        \"cv_mae_median_per_circuit\": float(best_cv_score),\n",
    "        \"test_mae\": float(test_mae),\n",
    "        \"test_rmse\": float(test_rmse),\n",
    "        \"test_r2\": float(test_r2),\n",
    "        \"test_median_mae_per_circuit\": float(median_mae_per_circuit),\n",
    "        \"n_iter\": int(n_iter),\n",
    "    }\n",
    "    model_json_path = os.path.join(OUTDIR, f\"hgb_{set_name.replace('+', '_')}_config.json\")\n",
    "    with open(model_json_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(f\"Saved: {model_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a0048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "HIST GRADIENT BOOSTING - COMPARISON\n",
      "====================================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hgb_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHIST GRADIENT BOOSTING - COMPARISON\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m100\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m hgb_summary = pd.DataFrame(\u001b[43mhgb_results\u001b[49m)\n\u001b[32m      8\u001b[39m hgb_summary_path = os.path.join(OUTDIR, \u001b[33m\"\u001b[39m\u001b[33mhgb_comparison.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m hgb_summary.to_csv(hgb_summary_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'hgb_results' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hist Gradient Boosting summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"HIST GRADIENT BOOSTING - COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "hgb_summary = pd.DataFrame(hgb_results)\n",
    "hgb_summary_path = os.path.join(OUTDIR, \"hgb_comparison.csv\")\n",
    "hgb_summary.to_csv(hgb_summary_path, index=False)\n",
    "\n",
    "print(\"\\n\" + hgb_summary[['block', 'test_MAE', 'test_RMSE', 'test_R2', 'n_iter']].to_string(index=False))\n",
    "\n",
    "baseline_hgb = hgb_summary.iloc[0]\n",
    "with_geom_hgb = hgb_summary.iloc[1]\n",
    "diff_mae_hgb = with_geom_hgb['test_MAE'] - baseline_hgb['test_MAE']\n",
    "pct_change_hgb = (diff_mae_hgb / baseline_hgb['test_MAE']) * 100\n",
    "\n",
    "print(f\"\\nBaseline MAE:      {baseline_hgb['test_MAE']:.4f}s\")\n",
    "print(f\"+Geometry MAE:     {with_geom_hgb['test_MAE']:.4f}s\")\n",
    "print(f\"Difference:        {diff_mae_hgb:+.4f}s ({pct_change_hgb:+.1f}%)\")\n",
    "\n",
    "if abs(diff_mae_hgb) < 0.0001:\n",
    "    print(\"→ Geometry: NEUTRAL\")\n",
    "elif diff_mae_hgb < 0:\n",
    "    print(f\"→ Geometry: HELPS by {abs(pct_change_hgb):.1f}%\")\n",
    "else:\n",
    "    print(f\"→ Geometry: HURTS by {pct_change_hgb:.1f}%\")\n",
    "\n",
    "print(f\"\\n✓ All HGB results saved to {OUTDIR}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d6ebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "CATBOOST - Circuit-Disjoint Evaluation (GroupKFold CV)\n",
      "====================================================================================================\n",
      "\n",
      "Hyperparameter grid - CatBoost: 128 configs (with early stopping)\n",
      "\n",
      "\n",
      "Tyre+Weather+State\n",
      "Tuning (33,420 train samples, 3 categorical features)...\n",
      "  Running GroupKFold CV (5 splits, 128 configs, grouped by CIRCUIT)...\n",
      "    [50/128] Best CV: 0.3527s\n",
      "    [100/128] Best CV: 0.3527s\n",
      "    [128/128] Best CV: 0.3527s\n",
      "  ✓ Best CV score: 0.3527s\n",
      "\n",
      "Results:\n",
      "  CV MAE (median per-circuit): 0.3527s\n",
      "  Test MAE:                    1.0259s\n",
      "  Test RMSE:                   1.6625s\n",
      "  Test R²:                     -6.7519\n",
      "  Test Median MAE/circuit:     0.3772s\n",
      "  Trees used:                  2998\n",
      "Saved: csv_output/nonlinear/catboost_Tyre_Weather_State_config.json\n",
      "\n",
      "Tyre+Weather+State+Geometry\n",
      "Tuning (33,420 train samples, 3 categorical features)...\n",
      "  Running GroupKFold CV (5 splits, 128 configs, grouped by CIRCUIT)...\n",
      "    [50/128] Best CV: 0.3498s\n",
      "    [100/128] Best CV: 0.3498s\n",
      "    [128/128] Best CV: 0.3498s\n",
      "  ✓ Best CV score: 0.3498s\n",
      "\n",
      "Results:\n",
      "  CV MAE (median per-circuit): 0.3498s\n",
      "  Test MAE:                    0.8476s\n",
      "  Test RMSE:                   1.3337s\n",
      "  Test R²:                     -3.9887\n",
      "  Test Median MAE/circuit:     0.3996s\n",
      "  Trees used:                  3000\n",
      "Saved: csv_output/nonlinear/catboost_Tyre_Weather_State_Geometry_config.json\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CATBOOST REGRESSOR\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"CATBOOST - Circuit-Disjoint Evaluation (GroupKFold CV)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# =========================================================\n",
    "# HELPER FUNCTION FOR CATBOOST (no sample_weight in sklearn pipeline)\n",
    "# =========================================================\n",
    "def tune_catboost_with_groupkfold_cv(param_grid, X_trainval, y_trainval, circuit_trainval, \n",
    "                                      n_splits=5, categorical_features=None):\n",
    "    \"\"\"\n",
    "    GroupKFold CV for CatBoost (natively handles categorical features).\n",
    "    Returns best params based on median MAE per circuit across folds.\n",
    "    Groups by CIRCUIT for unseen-circuit evaluation.\n",
    "    \"\"\"\n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=min(n_splits, circuit_trainval.nunique()))\n",
    "    n_configs = len(list(ParameterGrid(param_grid)))\n",
    "    \n",
    "    print(f\"  Running GroupKFold CV ({n_splits} splits, {n_configs} configs, grouped by CIRCUIT)...\")\n",
    "    \n",
    "    for i, params in enumerate(ParameterGrid(param_grid), 1):\n",
    "        fold_scores = []\n",
    "        \n",
    "        for tr_idx, va_idx in gkf.split(X_trainval, y_trainval, groups=circuit_trainval):\n",
    "            # Prepare data for this fold\n",
    "            X_tr = X_trainval.iloc[tr_idx].copy()\n",
    "            y_tr = y_trainval.iloc[tr_idx].copy()\n",
    "            X_va = X_trainval.iloc[va_idx].copy()\n",
    "            y_va = y_trainval.iloc[va_idx].copy()\n",
    "            \n",
    "            # Train CatBoost with early stopping\n",
    "            model = CatBoostRegressor(\n",
    "                random_state=42,\n",
    "                verbose=0,\n",
    "                iterations=3000,\n",
    "                early_stopping_rounds=100,\n",
    "                **params\n",
    "            )\n",
    "            \n",
    "            # Fit with validation set and categorical features\n",
    "            if categorical_features:\n",
    "                model.fit(\n",
    "                    X_tr, y_tr,\n",
    "                    cat_features=categorical_features,\n",
    "                    eval_set=(X_va, y_va),\n",
    "                    use_best_model=True,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(\n",
    "                    X_tr, y_tr,\n",
    "                    eval_set=(X_va, y_va),\n",
    "                    use_best_model=True,\n",
    "                    verbose=False\n",
    "                )\n",
    "            \n",
    "            # Predict on validation fold\n",
    "            yp = model.predict(X_va)\n",
    "            \n",
    "            # Compute per-circuit MAE for this fold\n",
    "            circuit_va = circuit_trainval.iloc[va_idx]\n",
    "            per_circuit = []\n",
    "            for cid in circuit_va.unique():\n",
    "                mask_circuit = (circuit_va == cid).values\n",
    "                if mask_circuit.sum() > 0:\n",
    "                    per_circuit.append(mean_absolute_error(\n",
    "                        y_va.values[mask_circuit], \n",
    "                        yp[mask_circuit]\n",
    "                    ))\n",
    "            \n",
    "            fold_scores.append(float(np.median(per_circuit)))\n",
    "        \n",
    "        # Median across folds\n",
    "        score = float(np.median(fold_scores))\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "        \n",
    "        if i % 50 == 0 or i == n_configs:\n",
    "            print(f\"    [{i}/{n_configs}] Best CV: {best_score:.4f}s\")\n",
    "    \n",
    "    print(f\"  ✓ Best CV score: {best_score:.4f}s\")\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# HYPERPARAMETER GRID FOR CATBOOST\n",
    "# =========================================================\n",
    "# param_catboost = {\n",
    "#     \"depth\": [4, 6, 8],\n",
    "#     \"learning_rate\": [0.05, 0.1],\n",
    "#     \"l2_leaf_reg\": [1, 3, 5],\n",
    "# }\n",
    "\n",
    "# More extensive grid (commented, with early stopping iterations can be higher):\n",
    "param_catboost = {\n",
    "    \"depth\": [4, 6, 8, 10],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 10],\n",
    "    \"random_strength\": [0.5, 1.0],\n",
    "}\n",
    "\n",
    "n_configs_catboost = len(list(ParameterGrid(param_catboost)))\n",
    "print(f\"\\nHyperparameter grid - CatBoost: {n_configs_catboost} configs (with early stopping)\\n\")\n",
    "\n",
    "# =========================================================\n",
    "# SETUP (if running CatBoost separately)\n",
    "# =========================================================\n",
    "# Define feature sets and data if not already in memory\n",
    "try:\n",
    "    feat_baseline\n",
    "except NameError:\n",
    "    print(\"Setting up features and data...\")\n",
    "    \n",
    "    # Load data\n",
    "    df_train = pd.read_excel(TRAIN_PATH)\n",
    "    df_val = pd.read_excel(VAL_PATH)\n",
    "    df_test = pd.read_excel(TEST_PATH)\n",
    "    \n",
    "    # Ensure is_new_tyre\n",
    "    for df in [df_train, df_val, df_test]:\n",
    "        if 'is_new_tyre' not in df.columns and 'TyreAgeAtStart' in df.columns:\n",
    "            df['is_new_tyre'] = (df['TyreAgeAtStart'] == 0).astype(int)\n",
    "    \n",
    "    # Create circuit_id for grouping\n",
    "    for df in [df_train, df_val, df_test]:\n",
    "        df['race_id'] = (\n",
    "            df['year'].astype(str) + '_' + \n",
    "            df['round'].astype(str) + '_' + \n",
    "            df['name'].astype(str)\n",
    "        )\n",
    "        df['circuit_id'] = df['name'].astype(str)\n",
    "    \n",
    "    # Prepare features\n",
    "    num_tyre = keep_existing(NUM_TYRE, df_train)\n",
    "    num_weather = keep_existing(NUM_WEATHER, df_train)\n",
    "    num_state = keep_existing(NUM_STATE, df_train)\n",
    "    num_geometry = [c for c in GEOM_COLS_ALL if c in df_train.columns and c != \"hb_at_end_of_max\"]\n",
    "    CAT = keep_existing(CAT_FEATURES, df_train)\n",
    "    \n",
    "    feat_baseline = num_tyre + num_weather + num_state\n",
    "    feat_with_geom = num_tyre + num_weather + num_state + num_geometry\n",
    "    \n",
    "    # Combine train+val\n",
    "    df_trainval = pd.concat([df_train, df_val], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Sort test\n",
    "    sort_cols = ['year', 'round', 'name']\n",
    "    if 'lap_number' in df_test.columns:\n",
    "        sort_cols.append('lap_number')\n",
    "    elif 'LapNumber' in df_test.columns:\n",
    "        sort_cols.append('LapNumber')\n",
    "    df_test_sorted = df_test.sort_values(sort_cols).reset_index(drop=True)\n",
    "    \n",
    "    print(\"✓ Features and data setup complete\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# RUN CATBOOST EXPERIMENTS (using same feature sets as DT/RF/HGB)\n",
    "# =========================================================\n",
    "catboost_results = []\n",
    "\n",
    "for set_name, num_feats in [(\"Tyre+Weather+State\", feat_baseline), \n",
    "                             (\"Tyre+Weather+State+Geometry\", feat_with_geom)]:\n",
    "    print(f\"\\n{set_name}\")\n",
    "    \n",
    "    # For CatBoost, we work with raw data (not preprocessed)\n",
    "    # Separate numerical and categorical\n",
    "    X_trainval_raw = df_trainval[num_feats + CAT].copy()\n",
    "    y_trainval_raw = df_trainval[TARGET].astype(float).copy()\n",
    "    circuit_trainval_raw = df_trainval['circuit_id'].copy()\n",
    "    \n",
    "    mask_trainval = y_trainval_raw.notna()\n",
    "    X_trainval_raw = X_trainval_raw.loc[mask_trainval]\n",
    "    y_trainval_raw = y_trainval_raw.loc[mask_trainval]\n",
    "    circuit_trainval_raw = circuit_trainval_raw.loc[mask_trainval]\n",
    "    \n",
    "    # Ensure categorical columns are strings (CatBoost requirement)\n",
    "    for c in CAT:\n",
    "        if c in X_trainval_raw.columns:\n",
    "            X_trainval_raw[c] = X_trainval_raw[c].astype(str)\n",
    "    \n",
    "    # Get categorical feature indices\n",
    "    cat_indices = [X_trainval_raw.columns.get_loc(c) for c in CAT if c in X_trainval_raw.columns]\n",
    "    \n",
    "    print(f\"Tuning ({len(X_trainval_raw):,} train samples, {len(cat_indices)} categorical features)...\")\n",
    "    best_params, best_cv_score = tune_catboost_with_groupkfold_cv(\n",
    "        param_grid=param_catboost,\n",
    "        X_trainval=X_trainval_raw,\n",
    "        y_trainval=y_trainval_raw,\n",
    "        circuit_trainval=circuit_trainval_raw,\n",
    "        n_splits=N_CV_SPLITS,\n",
    "        categorical_features=cat_indices if cat_indices else None\n",
    "    )\n",
    "    \n",
    "    # Refit and test\n",
    "    final_model = CatBoostRegressor(\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "        iterations=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        **best_params\n",
    "    )\n",
    "    \n",
    "    # Prepare validation data for early stopping in final refit\n",
    "    X_val_raw = df_val[num_feats + CAT].copy()\n",
    "    y_val_raw = df_val[TARGET].astype(float).copy()\n",
    "    \n",
    "    # Ensure categorical columns are strings in validation set\n",
    "    for c in CAT:\n",
    "        if c in X_val_raw.columns:\n",
    "            X_val_raw[c] = X_val_raw[c].astype(str)\n",
    "    \n",
    "    # Apply mask for valid targets in validation set\n",
    "    mask_val = y_val_raw.notna()\n",
    "    X_val_raw = X_val_raw.loc[mask_val]\n",
    "    y_val_raw = y_val_raw.loc[mask_val]\n",
    "    \n",
    "    # Compute circuit-balanced sample weights\n",
    "    vc = circuit_trainval_raw.value_counts()\n",
    "    w_trainval = circuit_trainval_raw.map(lambda c: 1.0 / vc.loc[c]).values\n",
    "    \n",
    "    # Prepare test data with same categorical handling\n",
    "    X_test_raw = df_test_sorted[num_feats + CAT].copy()\n",
    "    y_test_raw = df_test_sorted[TARGET].astype(float).copy()\n",
    "    \n",
    "    # Ensure categorical columns are strings in test set too\n",
    "    for c in CAT:\n",
    "        if c in X_test_raw.columns:\n",
    "            X_test_raw[c] = X_test_raw[c].astype(str)\n",
    "    \n",
    "    # Fit with validation set, early stopping, and circuit-balanced weights\n",
    "    if cat_indices:\n",
    "        final_model.fit(\n",
    "            X_trainval_raw, y_trainval_raw,\n",
    "            cat_features=cat_indices,\n",
    "            sample_weight=w_trainval,\n",
    "            eval_set=(X_val_raw, y_val_raw),\n",
    "            use_best_model=True,\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        final_model.fit(\n",
    "            X_trainval_raw, y_trainval_raw,\n",
    "            sample_weight=w_trainval,\n",
    "            eval_set=(X_val_raw, y_val_raw),\n",
    "            use_best_model=True,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    test_pred_all = final_model.predict(X_test_raw)\n",
    "    \n",
    "    mask = y_test_raw.notna()\n",
    "    test_pred = test_pred_all[mask]\n",
    "    y_test_valid = y_test_raw[mask]\n",
    "    \n",
    "    test_mae = float(mean_absolute_error(y_test_valid, test_pred))\n",
    "    test_rmse = float(np.sqrt(mean_squared_error(y_test_valid, test_pred)))\n",
    "    test_r2 = float(r2_score(y_test_valid, test_pred))\n",
    "    \n",
    "    # Per-circuit MAE (using circuit_id for consistency with CV grouping)\n",
    "    test_circuit_col = 'circuit_id' if 'circuit_id' in df_test_sorted.columns else 'name'\n",
    "    per_circuit_mae = []\n",
    "    circuit_names = []\n",
    "    for circuit in df_test_sorted[test_circuit_col].unique():\n",
    "        circuit_mask = (df_test_sorted[test_circuit_col] == circuit).values\n",
    "        circuit_mask = circuit_mask & mask.values\n",
    "        if circuit_mask.sum() > 0:\n",
    "            circuit_mae = mean_absolute_error(\n",
    "                df_test_sorted[TARGET].values[circuit_mask], \n",
    "                test_pred_all[circuit_mask]\n",
    "            )\n",
    "            per_circuit_mae.append(circuit_mae)\n",
    "            circuit_names.append(str(circuit))\n",
    "    \n",
    "    median_mae_per_circuit = float(np.median(per_circuit_mae))\n",
    "    n_trees = final_model.tree_count_\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  CV MAE (median per-circuit): {best_cv_score:.4f}s\")\n",
    "    print(f\"  Test MAE:                    {test_mae:.4f}s\")\n",
    "    print(f\"  Test RMSE:                   {test_rmse:.4f}s\")\n",
    "    print(f\"  Test R²:                     {test_r2:.4f}\")\n",
    "    print(f\"  Test Median MAE/circuit:     {median_mae_per_circuit:.4f}s\")\n",
    "    print(f\"  Trees used:                  {n_trees}\")\n",
    "    \n",
    "    catboost_results.append({\n",
    "        \"block\": set_name,\n",
    "        \"n_features\": len(num_feats) + len(CAT),\n",
    "        \"CV_MAE_per_circuit\": best_cv_score,\n",
    "        \"test_MAE\": test_mae,\n",
    "        \"test_RMSE\": test_rmse,\n",
    "        \"test_R2\": test_r2,\n",
    "        \"test_median_MAE_per_circuit\": median_mae_per_circuit,\n",
    "        \"n_trees\": n_trees,\n",
    "        \"best_params\": json.dumps(best_params),\n",
    "    })\n",
    "    \n",
    "    # Save per-circuit MAE\n",
    "    per_circuit_df = pd.DataFrame({\n",
    "        'Circuit': circuit_names,\n",
    "        'MAE': per_circuit_mae\n",
    "    }).sort_values('MAE', ascending=False)\n",
    "    per_circuit_path = os.path.join(OUTDIR, f\"catboost_{set_name.replace('+', '_')}_per_circuit_mae.csv\")\n",
    "    per_circuit_df.to_csv(per_circuit_path, index=False)\n",
    "    \n",
    "    # Save model config as JSON\n",
    "    model_info = {\n",
    "        \"model\": \"CatBoost\",\n",
    "        \"feature_set\": set_name,\n",
    "        \"n_features\": len(num_feats) + len(CAT),\n",
    "        \"best_params\": best_params,\n",
    "        \"cv_mae_median_per_circuit\": float(best_cv_score),\n",
    "        \"test_mae\": float(test_mae),\n",
    "        \"test_rmse\": float(test_rmse),\n",
    "        \"test_r2\": float(test_r2),\n",
    "        \"test_median_mae_per_circuit\": float(median_mae_per_circuit),\n",
    "        \"n_trees\": int(n_trees),\n",
    "    }\n",
    "    model_json_path = os.path.join(OUTDIR, f\"catboost_{set_name.replace('+', '_')}_config.json\")\n",
    "    with open(model_json_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(f\"Saved: {model_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ea073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# CATBOOST SUMMARY\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CATBOOST - COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "catboost_summary = pd.DataFrame(catboost_results)\n",
    "catboost_summary_path = os.path.join(OUTDIR, \"catboost_comparison.csv\")\n",
    "catboost_summary.to_csv(catboost_summary_path, index=False)\n",
    "\n",
    "print(\"\\n\" + catboost_summary[['block', 'test_MAE', 'test_RMSE', 'test_R2', 'n_trees']].to_string(index=False))\n",
    "\n",
    "baseline_catboost = catboost_summary.iloc[0]\n",
    "with_geom_catboost = catboost_summary.iloc[1]\n",
    "diff_mae_catboost = with_geom_catboost['test_MAE'] - baseline_catboost['test_MAE']\n",
    "pct_change_catboost = (diff_mae_catboost / baseline_catboost['test_MAE']) * 100\n",
    "\n",
    "print(f\"\\nBaseline MAE:      {baseline_catboost['test_MAE']:.4f}s\")\n",
    "print(f\"+Geometry MAE:     {with_geom_catboost['test_MAE']:.4f}s\")\n",
    "print(f\"Difference:        {diff_mae_catboost:+.4f}s ({pct_change_catboost:+.1f}%)\")\n",
    "\n",
    "if abs(diff_mae_catboost) < 0.0001:\n",
    "    print(\"→ Geometry: NEUTRAL\")\n",
    "elif diff_mae_catboost < 0:\n",
    "    print(f\"→ Geometry: HELPS by {abs(pct_change_catboost):.1f}%\")\n",
    "else:\n",
    "    print(f\"→ Geometry: HURTS by {pct_change_catboost:.1f}%\")\n",
    "\n",
    "print(f\"\\n✓ All CatBoost results saved to {OUTDIR}/\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Formula1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
